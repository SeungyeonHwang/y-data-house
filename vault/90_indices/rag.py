#!/usr/bin/env python3
"""
DeepSeek RAG ì‹œìŠ¤í…œ - 98ê°œ ì˜ìƒ ìë§‰ì„ ìì—°ì–´ë¡œ ê²€ìƒ‰í•˜ê³  AIê°€ ë‹µë³€
ê°œë–¡ê°™ì´ ë§í•´ë„ ì°°ë–¡ê°™ì´ ì•Œì•„ë“£ëŠ” ì‹œìŠ¤í…œ v4.0 - HyDE + Query Rewriting + LLM Re-Ranker
í•˜ë“œì½”ë”© í•„í„° ì™„ì „ ì œê±°, LLMì´ ì°½ì˜ì ìœ¼ë¡œ ê´€ë ¨ì„± íŒë‹¨
"""

import os
import sys
from pathlib import Path
from dotenv import load_dotenv
import chromadb
from chromadb.config import Settings as ChromaSettings
from openai import OpenAI

# ---------------- ê°œì„ ëœ Prompt template ----------------
PROMPT_TEMPLATE = """ë‹¹ì‹ ì€ ì¼ë³¸ ë¶€ë™ì‚° íˆ¬ì ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

ì•„ë˜ëŠ” 98ê°œ ë„ì¿„ë¶€ë™ì‚° ì˜ìƒì—ì„œ ì°¾ì€ **ê°€ì¥ ê´€ë ¨ì„± ë†’ì€** ë‚´ìš©ë“¤ì…ë‹ˆë‹¤. ì§ì ‘ì ì¸ ì–¸ê¸‰ì´ ì—†ì–´ë„ **ë¹„ìŠ·í•œ íŒ¨í„´, íˆ¬ì ì›ì¹™, ì§€ì—­ íŠ¹ì„±**ì„ ë°”íƒ•ìœ¼ë¡œ ìµœëŒ€í•œ ë„ì›€ë˜ëŠ” ì¡°ì–¸ì„ ì œê³µí•˜ì„¸ìš”.

## ì»¨í…ìŠ¤íŠ¸ (ê´€ë ¨ ì˜ìƒë“¤)
{context}

## ì‚¬ìš©ì ì§ˆë¬¸
{query}

## ë‹µë³€ ì‘ì„± ì§€ì¹¨
1. **5ê°œ ì˜ìƒì˜ ì •ë³´ë§Œ í™œìš©**í•˜ì—¬ ì§‘ì¤‘ëœ ì¡°ì–¸ ì œê³µ
2. **5ê°œ ì´í•˜ í•µì‹¬ bullet**(`- `)ë¡œ ì‘ì„±í•˜ê³ , ê° bullet ëì— `[ì˜ìƒ n]` í‘œì‹œ
3. ì§ì ‘ ì–¸ê¸‰ì´ ì—†ì–´ë„ **"ë„ì¿„/ì‚¬ì´íƒ€ë§ˆ ì‚¬ë¡€ë¡œ ìœ ì¶”í•˜ë©´..."** ì‹ìœ¼ë¡œ ì—°ê²°í•´ì„œ ì¡°ì–¸
4. **êµ¬ì²´ì  ìˆ˜ì¹˜, ì§€ì—­ëª…, íˆ¬ì ì „ëµ**ì„ í¬í•¨í•˜ì—¬ ì‹¤ìš©ì„± ë†’ì´ê¸°
5. ë§ˆì§€ë§‰ì— `### ğŸ’¡ í•œ ì¤„ ìš”ì•½:` í˜•ì‹ìœ¼ë¡œ í•µì‹¬ ì •ë¦¬
6. **ë¬´ì¡°ê±´ ë„ì›€ë˜ëŠ” ë‹µë³€**ì„ ë§Œë“¤ì–´ì•¼ í•¨ - "ëª¨ë¥´ê² ë‹¤" ê¸ˆì§€

**ì¤‘ìš”**: ì˜ìƒì—ì„œ ì§ì ‘ ì–¸ê¸‰ë˜ì§€ ì•Šì€ ì§€ì—­ì´ë¼ë„, ë¹„ìŠ·í•œ íˆ¬ì íŒ¨í„´ì´ë‚˜ ì›ì¹™ì„ ì ìš©í•´ì„œ ì¡°ì–¸í•´ì£¼ì„¸ìš”.
"""

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ê²½ë¡œ ì„¤ì •
VAULT_ROOT = Path(__file__).parent.parent
CHROMA_PATH = VAULT_ROOT / "90_indices" / "chroma"

class SmartRAG:
    def __init__(self):
        """ìŠ¤ë§ˆíŠ¸ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        self.api_key = os.getenv("DEEPSEEK_API_KEY")
        if not self.api_key:
            raise ValueError("âŒ DEEPSEEK_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
        
        # DeepSeek í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.client = OpenAI(
            api_key=self.api_key,
            base_url="https://api.deepseek.com"
        )
        
        # ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        try:
            self.chroma_client = chromadb.PersistentClient(
                path=str(CHROMA_PATH),
                settings=ChromaSettings(anonymized_telemetry=False)
            )
            self.collection = self.chroma_client.get_collection(
                "video_transcripts",
                embedding_function=None  # ì´ë¯¸ ì„ë² ë”© ì €ì¥ë¨
            )
            print(f"âœ… ChromaDB ì—°ê²°ë¨: {len(self.collection.get()['ids'])}ê°œ ì˜ìƒ")
        except Exception as e:
            raise ValueError(f"âŒ ChromaDB ë¡œë“œ ì‹¤íŒ¨: {e}\n'make embed'ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")

    def generate_hyde_documents(self, query: str, n_docs=1):
        """HyDE: 1ê°œ ê°€ìƒ ë¬¸ì„œ ìƒì„± (ë²¡í„° ë¶„ì‚° ë°©ì§€)"""
        hyde_docs = []
        
        for i in range(n_docs):
            try:
                response = self.client.chat.completions.create(
                    model="deepseek-chat",
                    messages=[
                        {"role": "system", "content": "ë‹¹ì‹ ì€ ì¼ë³¸ ë¶€ë™ì‚° íˆ¬ì ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ì™„ë²½í•œ ë‹µë³€ì„ ë‹´ì€ 150í† í° ë‚´ì™¸ì˜ ê°€ìƒ ë¬¸ì„œë¥¼ ì‘ì„±í•˜ì„¸ìš”."},
                        {"role": "user", "content": f"ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•œ ì™„ë²½í•œ ë‹µë³€ì´ ë‹´ê¸´ ë¬¸ì„œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”: '{query}'\n\në‹µë³€ì—ëŠ” êµ¬ì²´ì ì¸ ìˆ˜ì¹˜, ì§€ì—­ëª…, íˆ¬ì ì „ëµì´ í¬í•¨ë˜ì–´ì•¼ í•©ë‹ˆë‹¤. ë³€í˜• {i+1}ë²ˆì§¸ ê´€ì ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”."}
                    ],
                    max_tokens=150,  # í† í° ê¸¸ì´ ì œí•œ
                    temperature=0.7 + (i * 0.2)  # ë‹¤ì–‘ì„±ì„ ìœ„í•œ ì˜¨ë„ ì¡°ì ˆ
                )
                
                hyde_doc = response.choices[0].message.content.strip()
                hyde_docs.append(hyde_doc)
                print(f"ğŸ¯ HyDE ë¬¸ì„œ {i+1} ìƒì„±: {hyde_doc[:60]}...")
                
            except Exception as e:
                print(f"âš ï¸ HyDE ë¬¸ì„œ {i+1} ìƒì„± ì‹¤íŒ¨: {e}")
                continue
        
        return hyde_docs if hyde_docs else [None]

    def rewrite_query(self, query: str, context_sample: str = ""):
        """Query Rewriting: ê²€ìƒ‰ ìµœì í™”ëœ ì§ˆë¬¸ìœ¼ë¡œ ì¬ì‘ì„± (60í† í° ì œí•œ)"""
        try:
            prompt = f"""ë‹¹ì‹ ì€ ê²€ìƒ‰ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ê²€ìƒ‰ ì—”ì§„ì´ ì´í•´í•˜ê¸° ì‰¬ìš´ í˜•íƒœë¡œ ì¬ì‘ì„±í•˜ì„¸ìš”.

## ì›ë³¸ ì§ˆë¬¸
{query}

## ì»¨í…ìŠ¤íŠ¸ ìƒ˜í”Œ
{context_sample[:200]}

### ì§€ì‹œì‚¬í•­
ì›ë³¸ ì§ˆë¬¸ì„ ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¡œ ë³€í™˜í•˜ì„¸ìš”:
1. í•µì‹¬ í‚¤ì›Œë“œ + í•„í„°ê°€ í¬í•¨ëœ ê²€ìƒ‰ ì¿¼ë¦¬
2. êµ¬ì²´ì ì¸ ì¡°ê±´ê³¼ ìš©ì–´ê°€ ëª…í™•í•œ ì§ˆë¬¸

ì˜ˆì‹œ: "ì¢‹ì€ ì§€ì—­?" â†’ "ë„ì¿„ ì‚¬ì´íƒ€ë§ˆ ìˆ˜ìµë¥  ë†’ì€ ë¶€ë™ì‚° íˆ¬ì ì§€ì—­ ì¶”ì²œ"
**60í† í° ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.**
"""
            
            response = self.client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ê²€ìƒ‰ ì§ˆì˜ ìµœì í™” ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=60,  # í† í° ê¸¸ì´ ì œí•œ ê°•í™”
                temperature=0.5
            )
            
            rewritten = response.choices[0].message.content.strip()
            print(f"ğŸ”„ Query Rewriting: {rewritten}")
            return rewritten
            
        except Exception as e:
            print(f"âš ï¸ Query Rewriting ì‹¤íŒ¨: {e}")
            return query  # fallback to original



    def multi_search_v3(self, query: str):
        """HyDE + Query Rewriting + ë‹¤ì¤‘ ê²€ìƒ‰ ì „ëµ (v3.0 - Precision ë³´ì •)"""
        all_results = []
        
        # 1ì°¨: ì›ë³¸ ì§ˆë¬¸
        print(f"ğŸ” 1ì°¨ ê²€ìƒ‰: '{query}'")
        results1 = self._single_search(query, n_results=2)
        if results1:
            all_results.extend(self._format_results(results1, "ì›ë³¸ì§ˆë¬¸"))
        
        # 2ì°¨: HyDE (1ê°œ ë¬¸ì„œ ìƒì„±ìœ¼ë¡œ ë…¸ì´ì¦ˆ ê°ì†Œ)
        hyde_docs = self.generate_hyde_documents(query, n_docs=1)
        for i, hyde_doc in enumerate(hyde_docs):
            if hyde_doc:
                print(f"ğŸ” HyDE ê²€ìƒ‰")
                hyde_results = self._single_search(hyde_doc, n_results=2)  # 1ê°œ ë¬¸ì„œì§€ë§Œ 2ê°œ ê²°ê³¼
                if hyde_results:
                    all_results.extend(self._format_results(hyde_results, "HyDE"))
        
        # 3ì°¨: Query Rewriting (ì§ˆë¬¸ ì¬ì‘ì„± í›„ ê²€ìƒ‰)
        context_sample = all_results[0]['content'] if all_results else ""
        rewritten_query = self.rewrite_query(query, context_sample)
        if rewritten_query != query:
            print(f"ğŸ” Rewritten ê²€ìƒ‰")
            rewritten_results = self._single_search(rewritten_query, n_results=3)  # 2â†’3ìœ¼ë¡œ ì¦ê°€
            if rewritten_results:
                all_results.extend(self._format_results(rewritten_results, "Rewritten"))
        
        # 4ì°¨: LLM í‚¤ì›Œë“œ í™•ì¥ ê²€ìƒ‰ ì œê±° (LLM Re-Rankerë¡œ ëŒ€ì²´)
        
        # ì¤‘ë³µ ì œê±° ë° ì ìˆ˜ìˆœ ì •ë ¬
        unique_results = self._deduplicate_results(all_results)
        
        # LLM Re-Ranker: ì°½ì˜ì  ê´€ë ¨ì„± íŒë‹¨ìœ¼ë¡œ í•˜ë“œì½”ë”© í•„í„° ëŒ€ì²´
        if len(unique_results) > 5:
            # ìƒìœ„ 8ê°œë§Œ LLMì—ê²Œ ë³´ë‚´ì„œ í‰ê°€ (ë¹„ìš© ìµœì í™”)
            candidates = unique_results[:8]
            filtered_results = self._llm_rerank_filter(query, candidates)
        else:
            # ê²°ê³¼ê°€ ì ìœ¼ë©´ LLM í‰ê°€ í›„ ìœ ì‚¬ë„ë§Œìœ¼ë¡œ fallback
            filtered_results = self._llm_rerank_filter(query, unique_results)
            if len(filtered_results) < 2:
                print("âš ï¸ LLM í•„í„° ê²°ê³¼ ë¶€ì¡±, ìœ ì‚¬ë„ 0.25+ fallback")
                filtered_results = [r for r in unique_results if r['similarity'] > 0.25]  # 0.35 â†’ 0.25 ë³µì›
        
        print(f"ğŸ“Š LLM Re-Ranking: {len(unique_results)} â†’ {len(filtered_results)} (ì°½ì˜ì  ê´€ë ¨ì„± íŒë‹¨)")
        
        return filtered_results[:5]  # ìµœëŒ€ 5ê°œ ê³ í’ˆì§ˆ ê²°ê³¼

    def _single_search(self, query: str, n_results: int = 5):
        """ë‹¨ì¼ ê²€ìƒ‰ ì‹¤í–‰"""
        try:
            results = self.collection.query(
                query_texts=[query],
                n_results=n_results,
                include=["distances", "metadatas", "documents"]
            )
            return results if results["documents"][0] else None
        except Exception as e:
            print(f"âš ï¸ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return None

    def _format_results(self, results, search_type):
        """ê²€ìƒ‰ ê²°ê³¼ë¥¼ í†µí•© í¬ë§·ìœ¼ë¡œ ë³€í™˜ (duration ì •ë³´ ì¶”ê°€)"""
        formatted = []
        for i, (doc, metadata, distance) in enumerate(zip(
            results['documents'][0],
            results['metadatas'][0], 
            results['distances'][0]
        )):
            # duration ì •ë³´ ì¶”ì¶œ (ì•ˆì „í•˜ê²Œ)
            duration = metadata.get('duration', 'N/A') if metadata else 'N/A'
            
            formatted.append({
                'video_id': metadata.get('video_id', 'unknown') if metadata else 'unknown',
                'title': metadata.get('title', 'Unknown Title') if metadata else 'Unknown Title',
                'content': doc,
                'metadata': metadata if metadata else {},
                'distance': distance,
                'search_type': search_type,
                'similarity': 1 - distance,
                'duration': duration
            })
        return formatted

    def _deduplicate_results(self, all_results):
        """video_id ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì œê±° ë° ìµœê³  ì ìˆ˜ ìœ ì§€"""
        seen_videos = {}
        for result in all_results:
            video_id = result['video_id']
            if video_id not in seen_videos or result['similarity'] > seen_videos[video_id]['similarity']:
                seen_videos[video_id] = result
        
        # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬
        return sorted(seen_videos.values(), key=lambda x: x['similarity'], reverse=True)

    def _llm_rerank_filter(self, query: str, candidates: list):
        """LLM Re-Ranker: ì°½ì˜ì  ê´€ë ¨ì„± íŒë‹¨ìœ¼ë¡œ ì˜ìƒ í•„í„°ë§"""
        if not candidates:
            return []
        
        try:
            # í›„ë³´ ì˜ìƒë“¤ì„ LLMì—ê²Œ í‰ê°€ ìš”ì²­
            candidate_info = []
            for i, result in enumerate(candidates):
                candidate_info.append(
                    f"ì˜ìƒ {i+1}: {result['title']}\n"
                    f"ë‚´ìš©: {result['content'][:200]}...\n"
                    f"ìœ ì‚¬ë„: {result['similarity']:.3f}"
                )
            
            candidates_text = "\n---\n".join(candidate_info)
            
            prompt = f"""ë‹¹ì‹ ì€ ì¼ë³¸ ë¶€ë™ì‚° íˆ¬ì ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì•„ë˜ ì˜ìƒë“¤ì´ ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ì–¼ë§ˆë‚˜ ê´€ë ¨ì´ ìˆëŠ”ì§€ ì°½ì˜ì ìœ¼ë¡œ íŒë‹¨í•´ì£¼ì„¸ìš”.

## ì‚¬ìš©ì ì§ˆë¬¸
{query}

## í›„ë³´ ì˜ìƒë“¤
{candidates_text}

## í‰ê°€ ê¸°ì¤€
- ì§ì ‘ì  ê´€ë ¨ì„±: ì§ˆë¬¸ê³¼ ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ë‚´ìš© (9-10ì )
- ê°„ì ‘ì  ê´€ë ¨ì„±: ë¹„ìŠ·í•œ íˆ¬ì íŒ¨í„´ì´ë‚˜ ì›ì¹™ ì ìš© ê°€ëŠ¥ (7-8ì )
- ì°½ì˜ì  ì—°ê²°: ë‹¤ë¥¸ ì§€ì—­/ìƒí™©ì´ì§€ë§Œ ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ ê°€ëŠ¥ (6-7ì )
- ì•½ê°„ ê´€ë ¨: ë¶€ë™ì‚° íˆ¬ìì™€ ê´€ë ¨ì€ ìˆì§€ë§Œ ë„ì›€ ì œí•œì  (4-5ì )
- ë¬´ê´€: ë¶€ë™ì‚° íˆ¬ìì™€ ê´€ë ¨ ì—†ìŒ (1-3ì )

ê° ì˜ìƒì— ëŒ€í•´ 1-10ì ìœ¼ë¡œ í‰ê°€í•˜ê³ , 6ì  ì´ìƒë§Œ ì„ íƒí•˜ì„¸ìš”.
ì‘ë‹µ í˜•ì‹: "ì˜ìƒë²ˆí˜¸:ì ìˆ˜" (ì˜ˆ: 1:9, 3:8, 5:7)"""

            response = self.client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ê´€ë ¨ì„± í‰ê°€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì°½ì˜ì ì´ì§€ë§Œ ì •í™•í•œ íŒë‹¨ì„ ë‚´ë¦¬ì„¸ìš”."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=200,
                temperature=0.3  # ì¼ê´€ì„±ì„ ìœ„í•´ ë‚®ì€ ì˜¨ë„
            )
            
            # LLM ì‘ë‹µ íŒŒì‹±
            llm_response = response.choices[0].message.content.strip()
            print(f"ğŸ¤– LLM ê´€ë ¨ì„± í‰ê°€: {llm_response}")
            
            # ì ìˆ˜ íŒŒì‹± ë° í•„í„°ë§ (ê°œì„ ëœ íŒŒì‹±)
            selected_indices = []
            import re
            
            # ì •ê·œì‹ìœ¼ë¡œ "ìˆ«ì:ì ìˆ˜" íŒ¨í„´ ì¶”ì¶œ (ê´„í˜¸ ì„¤ëª… ë¬´ì‹œ)
            pattern = r'(\d+):(\d+)'
            matches = re.findall(pattern, llm_response)
            
            for idx_str, score_str in matches:
                try:
                    idx = int(idx_str) - 1  # 1-based to 0-based
                    score = int(score_str)
                    if score >= 6 and 0 <= idx < len(candidates):
                        selected_indices.append(idx)
                        print(f"   âœ… ì˜ìƒ {idx+1}: {score}ì  ì„ íƒ")
                    else:
                        print(f"   âŒ ì˜ìƒ {idx+1}: {score}ì  ì œì™¸ (6ì  ë¯¸ë§Œ)")
                except ValueError:
                    continue
            
            # ì„ íƒëœ ì˜ìƒë“¤ ë°˜í™˜ (LLM ì ìˆ˜ ìˆœì„œ ìœ ì§€)
            filtered = [candidates[i] for i in selected_indices]
            
            print(f"ğŸ¯ LLM ì„ íƒ: {len(selected_indices)}ê°œ ì˜ìƒ (6ì  ì´ìƒ)")
            return filtered
            
        except Exception as e:
            print(f"âš ï¸ LLM Re-Ranker ì‹¤íŒ¨: {e}, ìœ ì‚¬ë„ 0.25+ fallback")
            # LLM ì‹¤íŒ¨ì‹œ ìœ ì‚¬ë„ë§Œìœ¼ë¡œ í•„í„°ë§ (0.35 â†’ 0.25 ë³µì›)
            return [r for r in candidates if r['similarity'] > 0.25]

    def generate_answer(self, query: str, search_results: list):
        """ê°œì„ ëœ ë‹µë³€ ìƒì„± - 5ê°œ ì˜ìƒìœ¼ë¡œ ì§‘ì¤‘ëœ ë¶„ì„"""
        if not search_results:
            return "âŒ ê´€ë ¨ ì˜ìƒì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± - ë” ì§‘ì¤‘ëœ ì •ë³´
        context_parts = []
        for i, result in enumerate(search_results):
            duration = result.get('duration', 'N/A')
            context_parts.append(
                f"[ì˜ìƒ {i+1}] {result['title']} (ê¸¸ì´: {duration}, ìœ ì‚¬ë„: {result['similarity']:.2f})\n"
                f"ì—…ë¡œë“œ: {result['metadata']['upload']} | ì±„ë„: {result['metadata']['channel']}\n"
                f"ë‚´ìš©: {result['content'][:800]}...\n"  # ë” ë§ì€ ì»¨í…ìŠ¤íŠ¸
            )
        
        context = "\n---\n".join(context_parts)
        # .format() ì—ì„œ ì¤‘ê´„í˜¸ ì¶©ëŒ ë°©ì§€
        context_safe = context.replace("{", "{{").replace("}", "}}")
        prompt = PROMPT_TEMPLATE.format(context=context_safe, query=query)

        try:
            response = self.client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {"role": "system", "content": "ë‹¹ì‹ ì€ ì¼ë³¸ ë¶€ë™ì‚° íˆ¬ì ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì œê³µëœ 5ê°œ ì˜ìƒì˜ ì •ë³´ë§Œì„ ìµœëŒ€í•œ í™œìš©í•˜ì—¬ ë„ì›€ë˜ëŠ” ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”. ì§ì ‘ ì–¸ê¸‰ì´ ì—†ì–´ë„ ë¹„ìŠ·í•œ íŒ¨í„´ì„ ìœ ì¶”í•´ì„œ ì¡°ì–¸í•˜ê³ , 'ëª¨ë¥´ê² ë‹¤'ëŠ” ë‹µë³€ì€ ì ˆëŒ€ ê¸ˆì§€ì…ë‹ˆë‹¤."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=2000,
                temperature=0.7  # ì°½ì˜ì  ì—°ê²°ì„ ìœ„í•´ ì ë‹¹í•œ ìˆ˜ì¤€
            )
            
            return response.choices[0].message.content
            
        except Exception as e:
            return f"âŒ DeepSeek API ì˜¤ë¥˜: {e}"

    def chat(self, query: str):
        """ìŠ¤ë§ˆíŠ¸ RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (v4.1 - ì„¸ì»¨ë“œì˜¤í”¼ë‹ˆì–¸ ë°˜ì˜)"""
        import time
        start_time = time.time()
        
        print(f"ğŸš€ HyDE + Rewriting RAG v4.1 ì‹œì‘: '{query}' (ì„¸ì»¨ë“œì˜¤í”¼ë‹ˆì–¸ ë°˜ì˜)")
        
        # HyDE + Query Rewriting + LLM Re-Ranker íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        search_results = self.multi_search_v3(query)
        
        if not search_results:
            print("âŒ ëª¨ë“  ê²€ìƒ‰ ì „ëµ ì‹¤íŒ¨")
            return "âŒ ê´€ë ¨ ì˜ìƒì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        print(f"âœ… {len(search_results)}ê°œ ê³ í’ˆì§ˆ ì˜ìƒ ë°œê²¬ (LLM ì°½ì˜ì  ê´€ë ¨ì„± íŒë‹¨)")
        
        # ë°œê²¬ëœ ì˜ìƒë“¤ ë¯¸ë¦¬ë³´ê¸° (ìœ ì‚¬ë„ ìƒì„¸ í‘œì‹œ)
        for i, result in enumerate(search_results):
            duration = result.get('duration', 'N/A')
            print(f"   {i+1}. [{result['similarity']:.3f}] {result['title'][:40]}... ({duration}, {result['search_type']})")
        
        # AI ë‹µë³€ ìƒì„±
        print("ğŸ¤– DeepSeek ìµœì¢… ë‹µë³€ ìƒì„± ì¤‘...")
        answer = self.generate_answer(query, search_results)
        
        # ì„±ëŠ¥ ë° ë¹„ìš© ëª¨ë‹ˆí„°ë§ ë¡œê¹…
        elapsed_time = time.time() - start_time
        estimated_tokens = len(query) + sum(len(r['content'][:800]) for r in search_results) + len(answer)
        print(f"ğŸ“Š ì„±ëŠ¥ ë¡œê·¸: {elapsed_time:.1f}ì´ˆ, ì¶”ì •í† í°: {estimated_tokens:,}, ì˜ìƒìˆ˜: {len(search_results)}")
        
        # ì°¸ê³  ì˜ìƒ ëª©ë¡ (ë” ìƒì„¸í•œ ì •ë³´)
        references = "\n\nğŸ“š **ì°¸ê³  ì˜ìƒ (v4.1 - ì„¸ì»¨ë“œì˜¤í”¼ë‹ˆì–¸ ë°˜ì˜):**\n"
        for i, result in enumerate(search_results):
            duration = result.get('duration', 'N/A')
            references += f"{i+1}. {result['title']} ({result['metadata']['upload']}, {duration}) - ìœ ì‚¬ë„ {result['similarity']:.1%} [{result['search_type']}]\n"
        
        return answer + references

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    if len(sys.argv) < 2:
        print("ì‚¬ìš©ë²•: python vault/90_indices/rag.py \"ì§ˆë¬¸\"")
        print("\nì˜ˆì‹œ:")
        print('python vault/90_indices/rag.py "í›„ì¿ ì˜¤ì¹´ëŠ” íˆ¬ìì²˜ë¡œ ì–´ë–»ê²Œ ìƒê°í•´"')
        print('python vault/90_indices/rag.py "ìˆ˜ìµë¥  ë†’ì€ ì§€ì—­ì€ ì–´ë””?"')
        print('python vault/90_indices/rag.py "18ë…„ì°¨ ì§ì¥ì¸ì˜ íˆ¬ì ì „ëµ"')
        return
    
    query = " ".join(sys.argv[1:])
    
    try:
        # ìŠ¤ë§ˆíŠ¸ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        rag = SmartRAG()
        
        # ì§ˆë¬¸-ë‹µë³€ ì‹¤í–‰
        answer = rag.chat(query)
        
        print("\n" + "="*70)
        print(f"ğŸ¯ ì§ˆë¬¸: {query}")
        print("="*70)
        print(answer)
        print("="*70)
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜: {e}")

if __name__ == "__main__":
    main() 