#!/usr/bin/env python3
"""
ì±„ë„ë³„ ë²¡í„° ë°ì´í„° ë¶„ì„ ì‹œìŠ¤í…œ - Y-Data-House ìë™ í”„ë¡¬í”„íŠ¸ ìƒì„±
"""

import os
import re
from pathlib import Path
from collections import Counter
from typing import Dict, List, Tuple, Optional
import chromadb
from chromadb.config import Settings as ChromaSettings
from datetime import datetime
import json


class ChannelAnalyzer:
    """ë²¡í„° ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ì±„ë„ íŠ¹ì„±ì„ ìë™ìœ¼ë¡œ ì¶”ì¶œí•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self, chroma_path: Path = None):
        """ì´ˆê¸°í™”"""
        self.chroma_path = chroma_path or Path(__file__).parent / "chroma"
        
        if not self.chroma_path.exists():
            raise ValueError(f"âŒ ChromaDB ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {self.chroma_path}")
        
        try:
            self.client = chromadb.PersistentClient(
                path=str(self.chroma_path),
                settings=ChromaSettings(anonymized_telemetry=False)
            )
            print(f"âœ… ChromaDB ì—°ê²°ë¨: {self.chroma_path}")
        except Exception as e:
            raise ValueError(f"âŒ ChromaDB ì—°ê²° ì‹¤íŒ¨: {e}")
    
    def sanitize_collection_name(self, name: str) -> str:
        """ChromaDB ì»¬ë ‰ì…˜ ì´ë¦„ ì •ë¦¬"""
        sanitized = re.sub(r'[^\wê°€-í£]', '_', name)
        sanitized = re.sub(r'_+', '_', sanitized).strip('_')
        return sanitized[:50] if sanitized else "unknown_channel"
    
    def _find_collection_by_channel_name(self, channel_name: str):
        """ì±„ë„ëª…ìœ¼ë¡œ ì‹¤ì œ ì»¬ë ‰ì…˜ ì°¾ê¸°"""
        try:
            collections = self.client.list_collections()
            
            for collection in collections:
                if collection.name.startswith("channel_"):
                    # ì»¬ë ‰ì…˜ì—ì„œ ìƒ˜í”Œ ë°ì´í„° ê°€ì ¸ì™€ì„œ ì±„ë„ëª… í™•ì¸
                    try:
                        sample = collection.get(limit=1, include=['metadatas'])
                        if sample['metadatas'] and sample['metadatas'][0]:
                            metadata_channel = sample['metadatas'][0].get('channel', '')
                            if metadata_channel == channel_name:
                                return collection
                    except:
                        continue
            
            print(f"ğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ë ‰ì…˜ë“¤:")
            for collection in collections:
                if collection.name.startswith("channel_"):
                    try:
                        sample = collection.get(limit=1, include=['metadatas'])
                        if sample['metadatas'] and sample['metadatas'][0]:
                            metadata_channel = sample['metadatas'][0].get('channel', 'ì•Œ ìˆ˜ ì—†ìŒ')
                            print(f"  - {collection.name} â†’ {metadata_channel}")
                    except:
                        print(f"  - {collection.name} â†’ ë©”íƒ€ë°ì´í„° í™•ì¸ ë¶ˆê°€")
            
            return None
        except Exception as e:
            print(f"âŒ ì»¬ë ‰ì…˜ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return None
    
    def list_available_channels_for_analysis(self) -> List[str]:
        """ë¶„ì„ ê°€ëŠ¥í•œ ì±„ë„ ëª©ë¡ ë°˜í™˜"""
        try:
            collections = self.client.list_collections()
            channels = []
            
            for collection in collections:
                if collection.name.startswith("channel_"):
                    try:
                        data = collection.get()
                        if data['metadatas'] and len(data['metadatas']) > 0:
                            channel_name = data['metadatas'][0].get('channel', 'Unknown')
                            if channel_name != 'Unknown':
                                channels.append(channel_name)
                    except Exception:
                        continue
            
            return sorted(list(set(channels)))
        except Exception as e:
            print(f"âš ï¸ ì±„ë„ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
    
    def analyze_channel_content(self, channel_name: str) -> Dict:
        """ì±„ë„ ë²¡í„° ë°ì´í„° ë¶„ì„í•˜ì—¬ íŠ¹ì„± ì¶”ì¶œ"""
        # ì‹¤ì œ ì»¬ë ‰ì…˜ëª…ì„ ì°¾ê¸° ìœ„í•´ ëª¨ë“  ì»¬ë ‰ì…˜ì„ í™•ì¸
        collection = self._find_collection_by_channel_name(channel_name)
        if not collection:
            print(f"âŒ '{channel_name}' ì±„ë„ì˜ ì»¬ë ‰ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return {}
        
        try:
            data = collection.get(include=['documents', 'metadatas'])
            
            if not data['documents']:
                print(f"âš ï¸ {channel_name} ì±„ë„ì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return {}
            
            print(f"ğŸ“Š {channel_name} ì±„ë„ ë¶„ì„ ì‹œì‘: {len(data['documents'])}ê°œ ë¬¸ì„œ")
            
            # 1. ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ
            keywords = self._extract_keywords(data['documents'])
            
            # 2. ì½˜í…ì¸  íŒ¨í„´ ë¶„ì„
            patterns = self._analyze_content_patterns(data['documents'])
            
            # 3. ì±„ë„ ë©”íƒ€ë°ì´í„° ë¶„ì„
            metadata_insights = self._analyze_metadata(data['metadatas'])
            
            # 4. í†¤ & ìŠ¤íƒ€ì¼ ë¶„ì„
            tone_analysis = self._analyze_tone(data['documents'])
            
            return {
                'channel_name': channel_name,
                'keywords': keywords,
                'content_patterns': patterns,
                'metadata_insights': metadata_insights,
                'tone_analysis': tone_analysis,
                'total_videos': len(set(m.get('video_id', '') for m in data['metadatas'] if m)),
                'total_documents': len(data['documents']),
                'analysis_timestamp': datetime.now().isoformat()
            }
            
        except Exception as e:
            print(f"âš ï¸ ì±„ë„ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {}
    
    def _extract_keywords(self, documents: List[str]) -> Dict[str, int]:
        """ë¬¸ì„œì—ì„œ ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ"""
        all_text = ' '.join(documents)
        
        # í•œê¸€ í‚¤ì›Œë“œ ì¶”ì¶œ (2-8ê¸€ì)
        korean_keywords = re.findall(r'[ê°€-í£]{2,8}', all_text)
        
        # ì˜ë¬¸ í‚¤ì›Œë“œ ì¶”ì¶œ (3-15ê¸€ì)
        english_keywords = re.findall(r'[A-Za-z]{3,15}', all_text.lower())
        
        # ìˆ«ì íŒ¨í„´ ì¶”ì¶œ (ìˆ˜ì¹˜ + ë‹¨ìœ„)
        number_patterns = re.findall(r'\d+[ë…„ì›”ì¼%ì–µë§Œì›í‰ë‹¬ì¸µë¶„]', all_text)
        
        # íŠ¹ìˆ˜ í‚¤ì›Œë“œ íŒ¨í„´
        special_patterns = re.findall(r'[ê°€-í£]+(?:íˆ¬ì|ë¶€ë™ì‚°|ìˆ˜ìµ|ì „ëµ|ë¶„ì„|ë§¤ë§¤|ì„ëŒ€)', all_text)
        
        # ë¹ˆë„ ê³„ì‚°
        all_keywords = korean_keywords + english_keywords + number_patterns + special_patterns
        keyword_counts = Counter(all_keywords)
        
        # ë¶ˆìš©ì–´ ì œê±°
        stopwords = {
            'ì´ê²ƒ', 'ê·¸ê²ƒ', 'ì €ê²ƒ', 'ì—¬ê¸°', 'ê±°ê¸°', 'ì €ê¸°', 'ì´ê±°', 'ê·¸ê±°', 'ì €ê±°',
            'ë•Œë¬¸', 'ê²½ìš°', 'ì •ë„', 'ì‹œê°„', 'ì‚¬ëŒ', 'ìƒê°', 'ë§ì”€', 'ì´ì•¼ê¸°',
            'this', 'that', 'have', 'been', 'will', 'with', 'from', 'they'
        }
        
        filtered_keywords = {k: v for k, v in keyword_counts.items() 
                           if k not in stopwords and v >= 2}
        
        # ìƒìœ„ 30ê°œ í‚¤ì›Œë“œ ë°˜í™˜
        return dict(Counter(filtered_keywords).most_common(30))
    
    def _analyze_content_patterns(self, documents: List[str]) -> Dict:
        """ì½˜í…ì¸  íŒ¨í„´ ë¶„ì„"""
        patterns = {
            'investment_terms': 0,
            'location_mentions': 0,
            'numerical_data': 0,
            'experience_sharing': 0,
            'analysis_depth': 'medium',
            'real_estate_focus': 0,
            'practical_tips': 0
        }
        
        # íŒ¨í„´ ë¶„ì„ì„ ìœ„í•œ í‚¤ì›Œë“œ ê·¸ë£¹
        investment_terms = ['íˆ¬ì', 'ìˆ˜ìµë¥ ', 'ë§¤ë§¤', 'ì„ëŒ€', 'ìì‚°', 'í¬íŠ¸í´ë¦¬ì˜¤', 'í€ë“œ', 'ë°°ë‹¹']
        locations = ['ë„ì¿„', 'ì˜¤ì‚¬ì¹´', 'êµí† ', 'ìš”ì½”í•˜ë§ˆ', 'ì‹œë¶€ì•¼', 'ì‹ ì£¼ì¿ ', 'í•˜ë¼ì£¼ì¿ ', 'ë¡¯í°ê¸°']
        experience_words = ['ê²½í—˜', 'ì‹¤ì œë¡œ', 'ì§ì ‘', 'í•´ë³´ë‹ˆ', 'ëŠë‚€ì ', 'í›„ê¸°', 'ì²´í—˜', 'ì‹¤ì „']
        real_estate_words = ['ë¶€ë™ì‚°', 'ì•„íŒŒíŠ¸', 'ì›ë£¸', 'ì˜¤í”¼ìŠ¤í…”', 'ìƒê°€', 'í† ì§€', 'ê±´ë¬¼']
        practical_words = ['ë°©ë²•', 'íŒ', 'ë…¸í•˜ìš°', 'ì „ëµ', 'ë¹„ë²•', 'ìš”ë ¹', 'ê¸°ë²•']
        
        for doc in documents:
            # íˆ¬ì ê´€ë ¨ ìš©ì–´
            patterns['investment_terms'] += sum(doc.count(term) for term in investment_terms)
            
            # ì§€ì—­ ì–¸ê¸‰
            patterns['location_mentions'] += sum(doc.count(loc) for loc in locations)
            
            # ìˆ˜ì¹˜ ë°ì´í„°
            patterns['numerical_data'] += len(re.findall(r'\d+[%ì–µë§Œì›í‰ë…„ë‹¬]', doc))
            
            # ê²½í—˜ ê³µìœ  í‘œí˜„
            patterns['experience_sharing'] += sum(doc.count(word) for word in experience_words)
            
            # ë¶€ë™ì‚° ì§‘ì¤‘ë„
            patterns['real_estate_focus'] += sum(doc.count(word) for word in real_estate_words)
            
            # ì‹¤ìš©ì  íŒ
            patterns['practical_tips'] += sum(doc.count(word) for word in practical_words)
        
        # ë¶„ì„ ê¹Šì´ íŒë‹¨
        total_docs = len(documents)
        if patterns['numerical_data'] > total_docs * 5 and patterns['investment_terms'] > total_docs * 3:
            patterns['analysis_depth'] = 'deep'
        elif patterns['numerical_data'] < total_docs * 1:
            patterns['analysis_depth'] = 'light'
        
        return patterns
    
    def _analyze_metadata(self, metadatas: List[Dict]) -> Dict:
        """ë©”íƒ€ë°ì´í„° ë¶„ì„"""
        insights = {
            'avg_duration': 0,
            'upload_frequency': 'unknown',
            'popular_topics': [],
            'recent_trends': [],
            'video_types': {}
        }
        
        if not metadatas or not any(metadatas):
            return insights
        
        # ë¹„ë””ì˜¤ ê¸¸ì´ í‰ê· 
        durations = []
        for m in metadatas:
            if m and m.get('duration'):
                try:
                    # "MM:SS" í˜•ì‹ì„ ì´ˆë¡œ ë³€í™˜
                    duration_str = str(m['duration'])
                    if ':' in duration_str:
                        parts = duration_str.split(':')
                        if len(parts) == 2:
                            minutes, seconds = int(parts[0]), int(parts[1])
                            durations.append(minutes * 60 + seconds)
                except:
                    continue
        
        if durations:
            insights['avg_duration'] = sum(durations) / len(durations)
        
        # ì¸ê¸° í† í”½
        all_topics = []
        for m in metadatas:
            if m and m.get('topic'):
                if isinstance(m['topic'], list):
                    all_topics.extend(m['topic'])
                else:
                    all_topics.append(str(m['topic']))
        
        if all_topics:
            topic_counts = Counter(all_topics)
            insights['popular_topics'] = [topic for topic, _ in topic_counts.most_common(5)]
        
        # ë¹„ë””ì˜¤ ìœ í˜• ë¶„ì„
        titles = [m.get('title', '') for m in metadatas if m and m.get('title')]
        type_keywords = {
            'ë¶„ì„': ['ë¶„ì„', 'ë¦¬ë·°', 'í‰ê°€'],
            'íŒ': ['íŒ', 'ë°©ë²•', 'ë…¸í•˜ìš°', 'ë¹„ë²•'],
            'ê²½í—˜ë‹´': ['í›„ê¸°', 'ê²½í—˜', 'ì²´í—˜', 'ì‹¤ì „'],
            'ë‰´ìŠ¤': ['ì†ë³´', 'ë‰´ìŠ¤', 'ì •ë³´', 'ì—…ë°ì´íŠ¸']
        }
        
        for title in titles:
            for vid_type, keywords in type_keywords.items():
                if any(keyword in title for keyword in keywords):
                    insights['video_types'][vid_type] = insights['video_types'].get(vid_type, 0) + 1
        
        return insights
    
    def _analyze_tone(self, documents: List[str]) -> Dict:
        """í†¤ & ìŠ¤íƒ€ì¼ ë¶„ì„"""
        tone_indicators = {
            'formal': ['ìŠµë‹ˆë‹¤', 'ë©ë‹ˆë‹¤', 'ìˆìŠµë‹ˆë‹¤', 'ê²ƒì…ë‹ˆë‹¤', 'ë“œë¦½ë‹ˆë‹¤'],
            'casual': ['í•´ìš”', 'ì´ì—ìš”', 'ê±°ì˜ˆìš”', 'ë„¤ìš”', 'ì–´ìš”'],
            'expert': ['ë¶„ì„', 'ë°ì´í„°', 'ì§€í‘œ', 'ì „ë¬¸ì ', 'ì—°êµ¬', 'ì¡°ì‚¬'],
            'practical': ['ì‹¤ì œ', 'ì§ì ‘', 'ê²½í—˜', 'íŒ', 'ë°©ë²•', 'ë…¸í•˜ìš°'],
            'enthusiastic': ['ì •ë§', 'ë„ˆë¬´', 'ëŒ€ë°•', 'ì™„ì „', 'ì§„ì§œ', 'ìµœê³ ']
        }
        
        tone_scores = {tone: 0 for tone in tone_indicators.keys()}
        total_words = 0
        
        for doc in documents:
            total_words += len(doc.split())
            for tone, indicators in tone_indicators.items():
                tone_scores[tone] += sum(doc.count(indicator) for indicator in indicators)
        
        # ìƒëŒ€ì  ì ìˆ˜ ê³„ì‚° (1000ë‹¨ì–´ ê¸°ì¤€)
        if total_words > 0:
            normalized_scores = {tone: (score * 1000) / total_words 
                               for tone, score in tone_scores.items()}
        else:
            normalized_scores = tone_scores
        
        # ì£¼ìš” í†¤ ê²°ì •
        primary_tone = max(normalized_scores, key=normalized_scores.get)
        
        return {
            'primary_tone': primary_tone,
            'tone_scores': tone_scores,
            'normalized_scores': normalized_scores,
            'style_description': self._generate_style_description(primary_tone, normalized_scores)
        }
    
    def _generate_style_description(self, primary_tone: str, tone_scores: Dict) -> str:
        """ìŠ¤íƒ€ì¼ ì„¤ëª… ìƒì„±"""
        style_map = {
            'formal': 'ì •ì¤‘í•˜ê³  ì „ë¬¸ì ì¸ ì–´íˆ¬',
            'casual': 'ì¹œê·¼í•˜ê³  í¸ì•ˆí•œ ëŒ€í™”ì²´',
            'expert': 'ë¶„ì„ì ì´ê³  ë°ì´í„° ì¤‘ì‹¬ì ì¸ ìŠ¤íƒ€ì¼',
            'practical': 'ì‹¤ìš©ì ì´ê³  ê²½í—˜ ì¤‘ì‹¬ì ì¸ ì ‘ê·¼',
            'enthusiastic': 'í™œê¸°ì°¨ê³  ì—´ì •ì ì¸ í‘œí˜„'
        }
        
        primary_desc = style_map.get(primary_tone, 'ê· í˜•ì¡íŒ ìŠ¤íƒ€ì¼')
        
        # ë³´ì¡° í†¤ ì‹ë³„
        secondary_tones = sorted(tone_scores.items(), key=lambda x: x[1], reverse=True)[1:3]
        secondary_elements = []
        
        for tone, score in secondary_tones:
            if score > 0.3:  # ì„ê³„ê°’ ì´ìƒì¸ ê²½ìš°ì—ë§Œ
                if tone == 'expert':
                    secondary_elements.append('ì „ë¬¸ì„±')
                elif tone == 'practical':
                    secondary_elements.append('ì‹¤ìš©ì„±')
                elif tone == 'enthusiastic':
                    secondary_elements.append('ì—´ì •')
        
        if secondary_elements:
            return f"{primary_desc}ì´ë©° {', '.join(secondary_elements)}ë„ ê°•ì¡°í•˜ëŠ” ìŠ¤íƒ€ì¼"
        else:
            return primary_desc
    
    def generate_auto_prompt(self, channel_analysis: Dict) -> Dict:
        """ì±„ë„ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìë™ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        if not channel_analysis:
            return self._get_default_prompt()
        
        channel_name = channel_analysis['channel_name']
        keywords = list(channel_analysis.get('keywords', {}).keys())[:10]
        patterns = channel_analysis.get('content_patterns', {})
        tone_analysis = channel_analysis.get('tone_analysis', {})
        metadata = channel_analysis.get('metadata_insights', {})
        
        # í˜ë¥´ì†Œë‚˜ ìƒì„±
        persona = self._generate_persona(patterns, tone_analysis, keywords)
        
        # ì „ë¬¸ ë¶„ì•¼ ê²°ì •
        expertise = self._determine_expertise(keywords, patterns)
        
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±
        system_prompt = f"""ë‹¹ì‹ ì€ {channel_name} ì±„ë„ì„ ëŒ€í‘œí•˜ëŠ” {expertise} ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

ì´ ì±„ë„ì˜ íŠ¹ì§•:
- ì£¼ìš” í‚¤ì›Œë“œ: {', '.join(keywords[:5])}
- ì½˜í…ì¸  ìŠ¤íƒ€ì¼: {tone_analysis.get('style_description', 'ì „ë¬¸ì ')}
- ë¶„ì„ ê¹Šì´: {patterns.get('analysis_depth', 'medium')}
- ì´ ì˜ìƒ ìˆ˜: {channel_analysis.get('total_videos', 0)}ê°œ

ë‹¹ì‹ ì˜ ì—­í• ì€ ì´ ì±„ë„ì˜ ì˜ìƒ ë‚´ìš©ë§Œì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì—ê²Œ ì •í™•í•˜ê³  ì‹¤ìš©ì ì¸ ì¡°ì–¸ì„ ì œê³µí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤."""

        # ë‹µë³€ ê·œì¹™ ìƒì„±
        rules = self._generate_rules(patterns, tone_analysis)
        
        # ì¶œë ¥ í˜•ì‹ ê²°ì •
        output_format = self._determine_output_format(patterns, tone_analysis)
        
        return {
            "version": 1,
            "channel_name": channel_name,
            "created_at": datetime.now().isoformat(),
            "auto_generated": True,
            "persona": persona,
            "tone": tone_analysis.get('style_description', 'ì „ë¬¸ì ì´ê³  ì‹¤ìš©ì ì¸ ìŠ¤íƒ€ì¼'),
            "system_prompt": system_prompt,
            "rules": rules,
            "output_format": output_format,
            "expertise_keywords": keywords[:10],
            "analysis_metadata": {
                "total_videos": channel_analysis.get('total_videos', 0),
                "total_documents": channel_analysis.get('total_documents', 0),
                "analysis_timestamp": channel_analysis.get('analysis_timestamp'),
                "content_patterns": patterns,
                "tone_scores": tone_analysis.get('normalized_scores', {})
            }
        }
    
    def _generate_persona(self, patterns: Dict, tone_analysis: Dict, keywords: List[str]) -> str:
        """íŒ¨í„´ ë¶„ì„ì„ ë°”íƒ•ìœ¼ë¡œ í˜ë¥´ì†Œë‚˜ ìƒì„±"""
        base_persona = "ì „ë¬¸ ì»¨í…ì¸  ë¶„ì„ê°€"
        
        # ë¶€ë™ì‚°/íˆ¬ì ì „ë¬¸ì„± íŒë‹¨
        if patterns.get('investment_terms', 0) > 30 or any('íˆ¬ì' in k for k in keywords[:5]):
            if patterns.get('real_estate_focus', 0) > 20:
                base_persona = "ë¶€ë™ì‚° íˆ¬ì ì „ë¬¸ê°€"
            else:
                base_persona = "íˆ¬ì ì „ë¬¸ê°€"
        
        # ê²½í—˜ ì¤‘ì‹¬ì„± ì¶”ê°€
        if patterns.get('experience_sharing', 0) > 20:
            base_persona += "ì´ë©° ì‹¤ì „ ê²½í—˜ì´ í’ë¶€í•œ ì»¨ì„¤í„´íŠ¸"
        
        # ë¶„ì„ ê¹Šì´ ì¶”ê°€
        if patterns.get('analysis_depth') == 'deep':
            base_persona += "ì´ë©° ë°ì´í„° ê¸°ë°˜ ë¶„ì„ì„ ì¤‘ì‹œí•˜ëŠ” ì „ë¬¸ê°€"
        
        # ì‹¤ìš©ì„± ì¶”ê°€
        if patterns.get('practical_tips', 0) > 15:
            base_persona += "ì´ë©° ì‹¤ìš©ì ì¸ ì¡°ì–¸ì„ ì œê³µí•˜ëŠ” ë©˜í† "
        
        return base_persona
    
    def _determine_expertise(self, keywords: List[str], patterns: Dict) -> str:
        """í‚¤ì›Œë“œì™€ íŒ¨í„´ì„ ë°”íƒ•ìœ¼ë¡œ ì „ë¬¸ ë¶„ì•¼ ê²°ì •"""
        # í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„ì•¼ íŒë‹¨
        real_estate_keywords = ['ë¶€ë™ì‚°', 'íˆ¬ì', 'ë§¤ë§¤', 'ì„ëŒ€', 'ì›ë£¸', 'ì•„íŒŒíŠ¸']
        finance_keywords = ['í€ë“œ', 'ì£¼ì‹', 'ìì‚°', 'ìˆ˜ìµë¥ ', 'ë°°ë‹¹']
        travel_keywords = ['ì—¬í–‰', 'ë§›ì§‘', 'ë¬¸í™”', 'ê´€ê´‘']
        
        if any(keyword in keywords for keyword in real_estate_keywords):
            return "ë¶€ë™ì‚° íˆ¬ì"
        elif any(keyword in keywords for keyword in finance_keywords):
            return "ìì‚° ê´€ë¦¬"
        elif any(keyword in keywords for keyword in travel_keywords):
            return "ë¼ì´í”„ìŠ¤íƒ€ì¼"
        else:
            # íŒ¨í„´ ê¸°ë°˜ íŒë‹¨
            if patterns.get('real_estate_focus', 0) > 10:
                return "ë¶€ë™ì‚°"
            elif patterns.get('investment_terms', 0) > 20:
                return "íˆ¬ì"
            else:
                return "ì¢…í•© ì •ë³´"
    
    def _generate_rules(self, patterns: Dict, tone_analysis: Dict) -> List[str]:
        """íŒ¨í„´ì— ë”°ë¥¸ ë‹µë³€ ê·œì¹™ ìƒì„±"""
        rules = [
            "ë°˜ë“œì‹œ ì´ ì±„ë„ì˜ ì •ë³´ë§Œ í™œìš©í•˜ì—¬ ë‹µë³€",
            "ëª¨ë¥´ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡ ê¸ˆì§€, 'ì •ë³´ ë¶€ì¡±' ëª…ì‹œ"
        ]
        
        # ìˆ˜ì¹˜ ë°ì´í„° ì¤‘ì‹¬ì„±
        if patterns.get('numerical_data', 0) > 50:
            rules.append("êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ì™€ ë°ì´í„°ë¥¼ í¬í•¨í•˜ì—¬ ë‹µë³€")
        
        # ê²½í—˜ ê³µìœ  ì¤‘ì‹¬ì„±
        if patterns.get('experience_sharing', 0) > 15:
            rules.append("ì‹¤ì œ ê²½í—˜ê³¼ ì‚¬ë¡€ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ì„¤ëª…")
        
        # ì‹¤ìš©ì  ì ‘ê·¼
        if tone_analysis.get('primary_tone') == 'practical' or patterns.get('practical_tips', 0) > 10:
            rules.append("ì‹¤í–‰ ê°€ëŠ¥í•œ êµ¬ì²´ì  ë‹¨ê³„ ì œì‹œ")
        
        # ì „ë¬¸ì„± ì¤‘ì‹œ
        if tone_analysis.get('primary_tone') == 'expert':
            rules.append("ì „ë¬¸ ìš©ì–´ ì‚¬ìš© ì‹œ ì„¤ëª… í¬í•¨")
        
        # ê¸°ë³¸ ë‹µë³€ êµ¬ì¡°
        rules.append("ë‹µë³€ êµ¬ì¡°: í•µì‹¬ ìš”ì•½ â†’ ê·¼ê±° â†’ ì‹¤í–‰ ë‹¨ê³„")
        
        return rules
    
    def _determine_output_format(self, patterns: Dict, tone_analysis: Dict) -> Dict:
        """íŒ¨í„´ì— ë”°ë¥¸ ì¶œë ¥ í˜•ì‹ ê²°ì •"""
        if patterns.get('analysis_depth') == 'deep':
            return {
                "structure": "ğŸš€ í•µì‹¬ ìš”ì•½ â†’ ğŸ“Š ë°ì´í„° ë¶„ì„ â†’ ğŸ“š ê·¼ê±°/ì¶œì²˜ â†’ ğŸ“ ì‹¤í–‰ ë‹¨ê³„ â†’ ğŸ’¡ í•œì¤„ ìš”ì•½",
                "max_bullets": 7,
                "include_video_links": True,
                "data_emphasis": True
            }
        elif patterns.get('experience_sharing', 0) > 20:
            return {
                "structure": "ğŸš€ í•µì‹¬ ìš”ì•½ â†’ ğŸ’¼ ì‹¤ì œ ê²½í—˜ â†’ ğŸ“š ê·¼ê±°/ì¶œì²˜ â†’ ğŸ“ ì‹¤í–‰ ê°€ì´ë“œ â†’ ğŸ’¡ í•œì¤„ ìš”ì•½",
                "max_bullets": 5,
                "include_video_links": True,
                "experience_emphasis": True
            }
        else:
            return {
                "structure": "ğŸš€ í•µì‹¬ ìš”ì•½ â†’ ğŸ“š ê·¼ê±°/ì¶œì²˜ â†’ ğŸ“ ì‹¤í–‰ ë‹¨ê³„ â†’ ğŸ’¡ í•œì¤„ ìš”ì•½",
                "max_bullets": 5,
                "include_video_links": True,
                "balanced_approach": True
            }
    
    def _get_default_prompt(self) -> Dict:
        """ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ë°˜í™˜"""
        return {
            "version": 1,
            "channel_name": "default",
            "persona": "YouTube ë¹„ë””ì˜¤ ë‚´ìš© ì „ë¬¸ ë¶„ì„ê°€",
            "tone": "ì¹œê·¼í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ìŠ¤íƒ€ì¼",
            "system_prompt": "ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ë¹„ë””ì˜¤ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ìœ ìš©í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”.",
            "rules": ["ë¹„ë””ì˜¤ ë‚´ìš© ê¸°ë°˜ ë‹µë³€", "ì •í™•í•œ ì •ë³´ ì œê³µ", "ì¹œì ˆí•œ í†¤ ìœ ì§€"],
            "output_format": {
                "structure": "ë‹µë³€ â†’ ê·¼ê±° â†’ ìš”ì•½",
                "max_bullets": 3,
                "include_video_links": False
            }
        }


def main():
    """í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    try:
        analyzer = ChannelAnalyzer()
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ ì±„ë„ ëª©ë¡
        channels = analyzer.list_available_channels_for_analysis()
        print(f"ğŸ“º ë¶„ì„ ê°€ëŠ¥í•œ ì±„ë„: {channels}")
        
        if channels:
            # ì²« ë²ˆì§¸ ì±„ë„ ë¶„ì„ í…ŒìŠ¤íŠ¸
            channel = channels[0]
            print(f"\nğŸ” {channel} ì±„ë„ ë¶„ì„ í…ŒìŠ¤íŠ¸...")
            
            analysis = analyzer.analyze_channel_content(channel)
            if analysis:
                print(f"âœ… ë¶„ì„ ì™„ë£Œ:")
                print(f"  í‚¤ì›Œë“œ: {list(analysis['keywords'].keys())[:5]}")
                print(f"  ì£¼ìš” í†¤: {analysis['tone_analysis']['primary_tone']}")
                print(f"  ë¶„ì„ ê¹Šì´: {analysis['content_patterns']['analysis_depth']}")
                
                # ìë™ í”„ë¡¬í”„íŠ¸ ìƒì„± í…ŒìŠ¤íŠ¸
                auto_prompt = analyzer.generate_auto_prompt(analysis)
                print(f"  í˜ë¥´ì†Œë‚˜: {auto_prompt['persona']}")
            
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜: {e}")


if __name__ == "__main__":
    main()