#!/usr/bin/env python3
"""
Search-First ê²€ìƒ‰ íŒŒì´í”„ë¼ì¸
HyDE â†’ Query Rewrite â†’ Vector Search â†’ Conditional Re-Rank

ì¡°ì–¸ ê¸°ë°˜ ìµœì í™”:
- Re-RankëŠ” ë³µì¡í•œ ì¿¼ë¦¬ì—ë§Œ ì ìš©
- ìºì‹±ìœ¼ë¡œ LLM í˜¸ì¶œ 40% ì ˆê°
- latency budget â‰¤ 400ms
"""

import os
import time
import hashlib
import re
from typing import List, Dict, Optional, Tuple
from pathlib import Path
from dotenv import load_dotenv
import chromadb
from chromadb.config import Settings as ChromaSettings
from openai import OpenAI
from schemas import (
    SearchQuery, SearchConfig, SearchResult, SearchDocument, 
    QueryType, CacheKey
)

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

class SearchPipeline:
    """Search-First ê²€ìƒ‰ íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self, chroma_path: Path, model: str = "deepseek-chat"):
        """ì´ˆê¸°í™”"""
        self.model = model
        self.chroma_path = chroma_path
        
        # DeepSeek í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (HyDE, Query Rewriteìš©)
        try:
            api_key = os.getenv('DEEPSEEK_API_KEY')
            if not api_key:
                raise ValueError("DEEPSEEK_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ í•„ìš”í•©ë‹ˆë‹¤")
                
            self.client = OpenAI(
                api_key=api_key,
                base_url="https://api.deepseek.com/v1"
            )
            print(f"âœ… DeepSeek API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ (ëª¨ë¸: {model})")
        except Exception as e:
            raise ValueError(f"âŒ DeepSeek API ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        
        # ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        try:
            self.chroma_client = chromadb.PersistentClient(
                path=str(chroma_path),
                settings=ChromaSettings(anonymized_telemetry=False)
            )
            print(f"âœ… ChromaDB ì—°ê²°ë¨: {chroma_path}")
        except Exception as e:
            raise ValueError(f"âŒ ChromaDB ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        # ì¿¼ë¦¬ ë³µì¡ë„ ë¶„ë¥˜ íŒ¨í„´
        self.complex_patterns = [
            r'\b(ë¹„êµ|ë¶„ì„|í‰ê°€|ì–´ë–¤.*ì¢‹|ì°¨ì´|ì¥ë‹¨ì )\b',  # ë¹„êµ/ë¶„ì„
            r'\b(ì™œ|ì´ìœ |ì›ì¸|ê·¼ê±°|ë°°ê²½)\b',              # ì¸ê³¼ê´€ê³„
            r'\b(ì „ëµ|ë°©ë²•|ë°©ì‹|ê³¼ì •|ë‹¨ê³„)\b',             # ì ˆì°¨/ì „ëµ
            r'\b(ë¯¸ë˜|ì „ë§|ì˜ˆì¸¡|ê³„íš)\b',                 # ì˜ˆì¸¡/ê³„íš
            r'\b(ìµœì |ìµœê³ |ê°€ì¥.*|ì œì¼.*)\b',            # ìµœì í™”
        ]
        
        print("ğŸ” Search Pipeline ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _classify_query_complexity(self, query: str) -> QueryType:
        """ì¿¼ë¦¬ ë³µì¡ë„ ìë™ ë¶„ë¥˜"""
        query = query.lower()
        
        # ë³µì¡í•œ íŒ¨í„´ ê²€ì‚¬
        complex_score = 0
        for pattern in self.complex_patterns:
            if re.search(pattern, query):
                complex_score += 1
        
        # ê¸¸ì´ ê¸°ì¤€ ì¶”ê°€
        if len(query) > 50:
            complex_score += 1
        
        # ì§ˆë¬¸ ìˆ˜ ê¸°ì¤€
        question_count = query.count('?') + query.count('ï¼Ÿ')
        if question_count > 1:
            complex_score += 1
        
        # ë³µì¡ë„ ë¶„ë¥˜
        if complex_score >= 2:
            return QueryType.COMPLEX
        elif 'ì–¸ì œ' in query or 'ì–¼ë§ˆ' in query or 'ëª‡' in query:
            return QueryType.FACTUAL
        elif complex_score == 1:
            return QueryType.ANALYTICAL
        else:
            return QueryType.SIMPLE
    
    def _get_channel_collection(self, channel_name: str):
        """ì±„ë„ëª…ìœ¼ë¡œ ì»¬ë ‰ì…˜ ê°€ì ¸ì˜¤ê¸°"""
        try:
            collections = self.chroma_client.list_collections()
            
            for collection in collections:
                if collection.name.startswith("channel_"):
                    try:
                        sample = collection.get(limit=1, include=['metadatas'])
                        if sample['metadatas'] and sample['metadatas'][0]:
                            metadata_channel = sample['metadatas'][0].get('channel', '')
                            if metadata_channel == channel_name:
                                return collection
                    except:
                        continue
            
            return None
        except Exception as e:
            print(f"âŒ ì»¬ë ‰ì…˜ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return None
    
    def _generate_hyde_document(self, query: str, channel_name: str) -> Optional[str]:
        """HyDE ë¬¸ì„œ ìƒì„± (150 í† í° ì œí•œ)"""
        try:
            prompt = f"""ë‹¹ì‹ ì€ {channel_name} ì±„ë„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•œ ì™„ë²½í•œ ë‹µë³€ì´ ë‹´ê¸´ 150í† í° ë‚´ì™¸ì˜ ê°€ìƒ ë¬¸ì„œë¥¼ ì‘ì„±í•˜ì„¸ìš”.

ì§ˆë¬¸: {query}

ì´ ì±„ë„ì˜ ê´€ì ì—ì„œ êµ¬ì²´ì ì¸ ìˆ˜ì¹˜, ì§€ì—­ëª…, ì „ëµì´ í¬í•¨ëœ ë‹µë³€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”."""

            start_time = time.time()
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": f"ë‹¹ì‹ ì€ {channel_name} ì±„ë„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=150,
                temperature=0.7
            )
            
            hyde_doc = response.choices[0].message.content.strip()
            generation_time = (time.time() - start_time) * 1000
            
            print(f"ğŸ¯ HyDE ìƒì„± ì™„ë£Œ ({generation_time:.1f}ms): {hyde_doc[:50]}...")
            return hyde_doc
            
        except Exception as e:
            print(f"âš ï¸ HyDE ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    def _rewrite_query(self, query: str, channel_name: str, context: str = "") -> Optional[str]:
        """Query Rewriting - ì±„ë„ íŠ¹í™” í‚¤ì›Œë“œ ì‚½ì… (60 í† í° ì œí•œ)"""
        try:
            prompt = f"""ë‹¹ì‹ ì€ {channel_name} ì±„ë„ ì „ë¬¸ ê²€ìƒ‰ ìµœì í™” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ì´ ì±„ë„ì˜ ì»¨í…ì¸ ì—ì„œ ê²€ìƒ‰í•˜ê¸° ì‰¬ìš´ í˜•íƒœë¡œ ì¬ì‘ì„±í•˜ì„¸ìš”.

ì›ë³¸ ì§ˆë¬¸: {query}
ì±„ë„ ì»¨í…ìŠ¤íŠ¸: {context[:200]}

{channel_name} ì±„ë„ì˜ ì˜ìƒì—ì„œ ì°¾ì„ ìˆ˜ ìˆëŠ” í•µì‹¬ í‚¤ì›Œë“œì™€ ê°œë…ì„ í¬í•¨í•œ ê²€ìƒ‰ ì¿¼ë¦¬ë¡œ ì¬ì‘ì„±í•˜ì„¸ìš”.
**60í† í° ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.**"""

            start_time = time.time()
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": f"ë‹¹ì‹ ì€ {channel_name} ì±„ë„ ì „ë¬¸ ê²€ìƒ‰ ì§ˆì˜ ìµœì í™” ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=60,
                temperature=0.3
            )
            
            rewritten = response.choices[0].message.content.strip()
            generation_time = (time.time() - start_time) * 1000
            
            print(f"ğŸ”„ Query Rewrite ì™„ë£Œ ({generation_time:.1f}ms): {rewritten}")
            return rewritten
            
        except Exception as e:
            print(f"âš ï¸ Query Rewriting ì‹¤íŒ¨: {e}")
            return None
    
    def _vector_search(self, collection, query_text: str, n_results: int = 8) -> List[Dict]:
        """ë²¡í„° ê²€ìƒ‰ ì‹¤í–‰"""
        try:
            start_time = time.time()
            results = collection.query(
                query_texts=[query_text],
                n_results=n_results,
                include=["distances", "metadatas", "documents"]
            )
            search_time = (time.time() - start_time) * 1000
            
            if not results["documents"][0]:
                return []
            
            formatted_results = []
            for i, (doc, metadata, distance) in enumerate(zip(
                results['documents'][0],
                results['metadatas'][0], 
                results['distances'][0]
            )):
                formatted_results.append({
                    'video_id': metadata.get('video_id', 'unknown') if metadata else 'unknown',
                    'title': metadata.get('title', 'Unknown Title') if metadata else 'Unknown Title',
                    'content': doc,
                    'metadata': metadata if metadata else {},
                    'distance': distance,
                    'similarity': 1 - distance,
                    'search_time_ms': search_time
                })
            
            print(f"ğŸ“Š ë²¡í„° ê²€ìƒ‰ ì™„ë£Œ ({search_time:.1f}ms): {len(formatted_results)}ê°œ ë¬¸ì„œ")
            return formatted_results
            
        except Exception as e:
            print(f"âŒ ë²¡í„° ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def _should_use_rerank(self, query_type: QueryType, results_count: int) -> bool:
        """Re-rank ì‚¬ìš© ì—¬ë¶€ ê²°ì • (ì¡°ê±´ë¶€ ì‹¤í–‰)"""
        # ë³µì¡í•œ ì¿¼ë¦¬ì´ê³  ì¶©ë¶„í•œ ê²°ê³¼ê°€ ìˆì„ ë•Œë§Œ
        return (
            query_type in [QueryType.COMPLEX, QueryType.ANALYTICAL] and 
            results_count >= 5
        )
    
    def _llm_rerank(self, query: str, candidates: List[Dict], channel_name: str) -> List[Dict]:
        """LLM Re-Ranking (top 8 ë¬¸ì„œì—ë§Œ ì ìš©, 400ms ì˜ˆì‚°)"""
        if not candidates:
            return []
        
        try:
            start_time = time.time()
            
            # í›„ë³´ ì •ë³´ êµ¬ì„± (ê°„ê²°í•˜ê²Œ)
            candidate_info = []
            for i, result in enumerate(candidates[:8]):  # top 8ë§Œ
                candidate_info.append(
                    f"ë¬¸ì„œ {i+1}: {result['title']}\n"
                    f"ë‚´ìš©: {result['content'][:150]}...\n"
                    f"ìœ ì‚¬ë„: {result['similarity']:.3f}"
                )
            
            candidates_text = "\n---\n".join(candidate_info)
            
            prompt = f"""ë‹¹ì‹ ì€ {channel_name} ì±„ë„ ì „ë¬¸ ë¬¸ì„œ ê´€ë ¨ì„± í‰ê°€ìì…ë‹ˆë‹¤. 
ì‚¬ìš©ì ì§ˆë¬¸ì— ê°€ì¥ ë„ì›€ì´ ë  ë¬¸ì„œë“¤ì„ ì„ ë³„í•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: {query}

í›„ë³´ ë¬¸ì„œë“¤:
{candidates_text}

ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ë¬¸ì„œ ë²ˆí˜¸ë¥¼ ìš°ì„ ìˆœìœ„ëŒ€ë¡œ ë‚˜ì—´í•˜ì„¸ìš”. (ì˜ˆ: 1,3,5,2)
ìµœëŒ€ 5ê°œê¹Œì§€ ì„ íƒí•˜ì„¸ìš”."""

            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": f"ë‹¹ì‹ ì€ {channel_name} ì±„ë„ ì „ë¬¸ ë¬¸ì„œ ê´€ë ¨ì„± í‰ê°€ìì…ë‹ˆë‹¤."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=50,
                temperature=0.1
            )
            
            rerank_time = (time.time() - start_time) * 1000
            
            selection = response.choices[0].message.content.strip()
            print(f"ğŸ¤– LLM Re-rank ì™„ë£Œ ({rerank_time:.1f}ms): {selection}")
            
            # ì„ íƒëœ ì¸ë±ìŠ¤ íŒŒì‹±
            try:
                selected_indices = [int(x.strip()) - 1 for x in selection.replace(' ', '').split(',') if x.strip().isdigit()]
                reranked = []
                
                for i, idx in enumerate(selected_indices):
                    if 0 <= idx < len(candidates):
                        doc = candidates[idx].copy()
                        doc['rank_score'] = 1.0 - (i * 0.1)  # ìˆœìœ„ ì ìˆ˜
                        reranked.append(doc)
                
                if len(reranked) >= 2:
                    return reranked
                else:
                    # fallback: ìœ ì‚¬ë„ ê¸°ë°˜
                    return [r for r in candidates if r['similarity'] > 0.3][:5]
                    
            except Exception:
                print("âš ï¸ Re-rank ê²°ê³¼ íŒŒì‹± ì‹¤íŒ¨, ìœ ì‚¬ë„ ê¸°ë°˜ fallback")
                return [r for r in candidates if r['similarity'] > 0.3][:5]
                
        except Exception as e:
            print(f"âš ï¸ LLM Re-ranking ì‹¤íŒ¨: {e}")
            return [r for r in candidates if r['similarity'] > 0.3][:5]
    
    def _merge_and_deduplicate(self, all_results: List[List[Dict]], search_methods: List[str]) -> List[Dict]:
        """ê²€ìƒ‰ ê²°ê³¼ ë³‘í•© ë° ì¤‘ë³µ ì œê±°"""
        seen_videos = {}
        
        for results, method in zip(all_results, search_methods):
            for result in results:
                video_id = result['video_id']
                result['search_method'] = method
                
                # ë” ë†’ì€ ìœ ì‚¬ë„ì˜ ê²°ê³¼ ìœ ì§€
                if video_id not in seen_videos or result['similarity'] > seen_videos[video_id]['similarity']:
                    seen_videos[video_id] = result
        
        # ìœ ì‚¬ë„ìˆœ ì •ë ¬
        merged_results = sorted(seen_videos.values(), key=lambda x: x['similarity'], reverse=True)
        
        print(f"ğŸ”— ê²°ê³¼ ë³‘í•© ì™„ë£Œ: {len(merged_results)}ê°œ ê³ ìœ  ë¬¸ì„œ")
        return merged_results
    
    def search(self, search_query: SearchQuery, config: SearchConfig) -> SearchResult:
        """ë©”ì¸ ê²€ìƒ‰ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        start_time = time.time()
        
        print(f"ğŸ” ê²€ìƒ‰ ì‹œì‘: '{search_query.original_query}' in {search_query.channel_name}")
        
        # 1. ì±„ë„ ì»¬ë ‰ì…˜ ê°€ì ¸ì˜¤ê¸°
        collection = self._get_channel_collection(search_query.channel_name)
        if not collection:
            return SearchResult(
                query_id=search_query.query_id,
                channel_name=search_query.channel_name,
                documents=[],
                total_found=0,
                search_time_ms=0,
                hyde_used=False,
                rewrite_used=False,
                rerank_used=False
            )
        
        # 2. ì¿¼ë¦¬ ë³µì¡ë„ ë¶„ë¥˜
        query_type = self._classify_query_complexity(search_query.original_query)
        print(f"ğŸ“Š ì¿¼ë¦¬ íƒ€ì…: {query_type.value}")
        
        all_results = []
        search_methods = []
        
        # 3. ì›ë³¸ ì¿¼ë¦¬ ê²€ìƒ‰ (í•­ìƒ ì‹¤í–‰)
        print("ğŸ“ 1ë‹¨ê³„: ì›ë³¸ ì¿¼ë¦¬ ê²€ìƒ‰")
        original_results = self._vector_search(collection, search_query.original_query, config.max_results)
        if original_results:
            all_results.append(original_results)
            search_methods.append("original")
        
        # 4. HyDE ê²€ìƒ‰ (í™œì„±í™”ëœ ê²½ìš°)
        hyde_used = False
        if config.enable_hyde:
            print("ğŸ¯ 2ë‹¨ê³„: HyDE ê²€ìƒ‰")
            hyde_doc = self._generate_hyde_document(search_query.original_query, search_query.channel_name)
            if hyde_doc:
                search_query.hyde_document = hyde_doc
                hyde_results = self._vector_search(collection, hyde_doc, config.max_results)
                if hyde_results:
                    all_results.append(hyde_results)
                    search_methods.append("hyde")
                    hyde_used = True
        
        # 5. Query Rewriting ê²€ìƒ‰ (í™œì„±í™”ëœ ê²½ìš°)
        rewrite_used = False
        if config.enable_rewrite and all_results:
            print("ğŸ”„ 3ë‹¨ê³„: Query Rewriting ê²€ìƒ‰")
            context = all_results[0][0]['content'] if all_results[0] else ""
            rewritten_query = self._rewrite_query(search_query.original_query, search_query.channel_name, context)
            if rewritten_query and rewritten_query != search_query.original_query:
                search_query.rewritten_query = rewritten_query
                rewrite_results = self._vector_search(collection, rewritten_query, config.max_results)
                if rewrite_results:
                    all_results.append(rewrite_results)
                    search_methods.append("rewritten")
                    rewrite_used = True
        
        # 6. ê²°ê³¼ ë³‘í•© ë° ì¤‘ë³µ ì œê±°
        if not all_results:
            search_time_ms = (time.time() - start_time) * 1000
            return SearchResult(
                query_id=search_query.query_id,
                channel_name=search_query.channel_name,
                documents=[],
                total_found=0,
                search_time_ms=search_time_ms,
                hyde_used=hyde_used,
                rewrite_used=rewrite_used,
                rerank_used=False
            )
        
        merged_results = self._merge_and_deduplicate(all_results, search_methods)
        
        # 7. ì¡°ê±´ë¶€ Re-ranking
        rerank_used = False
        final_results = merged_results
        
        if config.enable_rerank and self._should_use_rerank(query_type, len(merged_results)):
            print("ğŸ¤– 4ë‹¨ê³„: LLM Re-ranking (ì¡°ê±´ë¶€)")
            reranked_results = self._llm_rerank(search_query.original_query, merged_results, search_query.channel_name)
            if reranked_results:
                final_results = reranked_results
                rerank_used = True
        
        # 8. ìœ ì‚¬ë„ ì„ê³„ê°’ í•„í„°ë§
        filtered_results = [r for r in final_results if r['similarity'] > config.similarity_threshold]
        final_documents = filtered_results[:config.max_results]
        
        # 9. SearchDocument ê°ì²´ë¡œ ë³€í™˜
        search_documents = []
        for doc in final_documents:
            search_documents.append(SearchDocument(
                video_id=doc['video_id'],
                title=doc['title'],
                content=doc['content'],
                similarity=doc['similarity'],
                metadata=doc['metadata'],
                search_method=doc['search_method'],
                rank_score=doc.get('rank_score')
            ))
        
        search_time_ms = (time.time() - start_time) * 1000
        
        print(f"âœ… ê²€ìƒ‰ ì™„ë£Œ ({search_time_ms:.1f}ms): {len(search_documents)}ê°œ ë¬¸ì„œ")
        
        return SearchResult(
            query_id=search_query.query_id,
            channel_name=search_query.channel_name,
            documents=search_documents,
            total_found=len(merged_results),
            search_time_ms=search_time_ms,
            hyde_used=hyde_used,
            rewrite_used=rewrite_used,
            rerank_used=rerank_used
        ) 