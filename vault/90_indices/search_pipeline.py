#!/usr/bin/env python3
"""
Search-First ê²€ìƒ‰ íŒŒì´í”„ë¼ì¸
HyDE â†’ Query Rewrite â†’ Vector Search â†’ Conditional Re-Rank

ì¡°ì–¸ ê¸°ë°˜ ìµœì í™”:
- Re-RankëŠ” ë³µì¡í•œ ì¿¼ë¦¬ì—ë§Œ ì ìš©
- ìºì‹±ìœ¼ë¡œ LLM í˜¸ì¶œ 40% ì ˆê°
- latency budget â‰¤ 500ms (í†µì¼ëœ ëª©í‘œ)
"""

import os
import time
import hashlib
import re
from typing import List, Dict, Optional, Tuple
from pathlib import Path
from dotenv import load_dotenv
import chromadb
from chromadb.config import Settings as ChromaSettings
from openai import OpenAI
from schemas import (
    SearchQuery, SearchConfig, SearchResult, SearchDocument, 
    QueryType, CacheKey
)

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

class SearchPipeline:
    """Search-First ê²€ìƒ‰ íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self, chroma_path: Path, model: str = "deepseek-chat"):
        """ì´ˆê¸°í™”"""
        self.model = model
        self.chroma_path = chroma_path
        
        # DeepSeek í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (HyDE, Query Rewriteìš©)
        try:
            api_key = os.getenv('DEEPSEEK_API_KEY')
            if not api_key:
                raise ValueError("DEEPSEEK_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ í•„ìš”í•©ë‹ˆë‹¤")
                
            self.client = OpenAI(
                api_key=api_key,
                base_url="https://api.deepseek.com/v1"
            )
            print(f"âœ… DeepSeek API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ (ëª¨ë¸: {model})")
        except Exception as e:
            raise ValueError(f"âŒ DeepSeek API ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        
        # ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        try:
            self.chroma_client = chromadb.PersistentClient(
                path=str(chroma_path),
                settings=ChromaSettings(anonymized_telemetry=False)
            )
            print(f"âœ… ChromaDB ì—°ê²°ë¨: {chroma_path}")
        except Exception as e:
            raise ValueError(f"âŒ ChromaDB ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        # ì¿¼ë¦¬ ë³µì¡ë„ ë¶„ë¥˜ íŒ¨í„´
        self.complex_patterns = [
            r'\b(ë¹„êµ|ë¶„ì„|í‰ê°€|ì–´ë–¤.*ì¢‹|ì°¨ì´|ì¥ë‹¨ì )\b',  # ë¹„êµ/ë¶„ì„
            r'\b(ì™œ|ì´ìœ |ì›ì¸|ê·¼ê±°|ë°°ê²½)\b',              # ì¸ê³¼ê´€ê³„
            r'\b(ì „ëµ|ë°©ë²•|ë°©ì‹|ê³¼ì •|ë‹¨ê³„)\b',             # ì ˆì°¨/ì „ëµ
            r'\b(ë¯¸ë˜|ì „ë§|ì˜ˆì¸¡|ê³„íš)\b',                 # ì˜ˆì¸¡/ê³„íš
            r'\b(ìµœì |ìµœê³ |ê°€ì¥.*|ì œì¼.*)\b',            # ìµœì í™”
        ]
        
        print("ğŸ” Search Pipeline ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _get_relevance_category(self, similarity: float) -> str:
        """ìœ ì‚¬ë„ ì ìˆ˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì—°ê´€ì„± ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜"""
        if similarity >= 0.8:
            return "ë§¤ìš° ë†’ìŒ"
        elif similarity >= 0.6:
            return "ë†’ìŒ"
        elif similarity >= 0.4:
            return "ë³´í†µ"
        elif similarity >= 0.2:
            return "ë‚®ìŒ"
        else:
            return "ë§¤ìš° ë‚®ìŒ"
    
    def _classify_query_complexity(self, query: str) -> QueryType:
        """ì¿¼ë¦¬ ë³µì¡ë„ ìë™ ë¶„ë¥˜"""
        query = query.lower()
        
        # ë³µì¡í•œ íŒ¨í„´ ê²€ì‚¬
        complex_score = 0
        for pattern in self.complex_patterns:
            if re.search(pattern, query):
                complex_score += 1
        
        # ê¸¸ì´ ê¸°ì¤€ ì¶”ê°€
        if len(query) > 50:
            complex_score += 1
        
        # ì§ˆë¬¸ ìˆ˜ ê¸°ì¤€
        question_count = query.count('?') + query.count('ï¼Ÿ')
        if question_count > 1:
            complex_score += 1
        
        # ë³µì¡ë„ ë¶„ë¥˜
        if complex_score >= 2:
            return QueryType.COMPLEX
        elif 'ì–¸ì œ' in query or 'ì–¼ë§ˆ' in query or 'ëª‡' in query:
            return QueryType.FACTUAL
        elif complex_score == 1:
            return QueryType.ANALYTICAL
        else:
            return QueryType.SIMPLE
    
    def _select_pipeline_mode(self, query_type: QueryType, query: str) -> str:
        """ì¡°ê±´ë¶€ íŒŒì´í”„ë¼ì¸ ëª¨ë“œ ì„ íƒ (ì „ë¬¸ê°€ ì¡°ì–¸ ë°˜ì˜)"""
        
        # 1. ê²½ëŸ‰ íŒŒì´í”„ë¼ì¸: ê°„ë‹¨í•œ FAQ, ì‚¬ì‹¤í˜• ì§ˆë¬¸
        if query_type in [QueryType.SIMPLE, QueryType.FACTUAL]:
            # ë‹¨ìˆœí•œ í‚¤ì›Œë“œ ê²€ìƒ‰ì´ë‚˜ ì‚¬ì‹¤ í™•ì¸
            if len(query) <= 30 and ('ë¬´ì—‡' in query or 'ì–¸ì œ' in query or 'ì–¼ë§ˆ' in query):
                return "lightweight"
        
        # 2. ì¢…í•© íŒŒì´í”„ë¼ì¸: ë³µì¡í•œ ë¶„ì„, ë¹„êµ, ì „ëµ ì§ˆë¬¸
        if query_type == QueryType.COMPLEX:
            return "comprehensive"
        
        # ë³µì¡í•œ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ê²½ìš° ì¢…í•© íŒŒì´í”„ë¼ì¸
        complex_keywords = ['ë¹„êµ', 'ì°¨ì´ì ', 'ì¥ë‹¨ì ', 'ë¶„ì„', 'í‰ê°€', 'ì¶”ì²œ', 'ì „ëµ', 
                           'ë°©ë²•', 'ê³¼ì •', 'ì ˆì°¨', 'ì´ìœ ', 'ì›ì¸', 'ë°°ê²½', 'ì˜í–¥', 
                           'ë¯¸ë˜', 'ì „ë§', 'ì˜ˆì¸¡', 'ê³ ë ¤ì‚¬í•­', 'vs']
        
        if any(keyword in query for keyword in complex_keywords):
            return "comprehensive"
        
        # ë‹¤ì¤‘ ì§ˆë¬¸ì´ë‚˜ ê¸´ ì§ˆë¬¸
        if len(query) > 60 or query.count('?') > 1 or query.count('ï¼Ÿ') > 1:
            return "comprehensive"
        
        # 3. í‘œì¤€ íŒŒì´í”„ë¼ì¸: ë‚˜ë¨¸ì§€ (ë¶„ì„í˜•, ì¤‘ê°„ ë³µì¡ë„)
        return "standard"
    
    def _get_channel_collection(self, channel_name: str):
        """ì±„ë„ëª…ìœ¼ë¡œ ì»¬ë ‰ì…˜ ê°€ì ¸ì˜¤ê¸°"""
        try:
            collections = self.chroma_client.list_collections()
            
            for collection in collections:
                if collection.name.startswith("channel_"):
                    try:
                        sample = collection.get(limit=1, include=['metadatas'])
                        if sample['metadatas'] and sample['metadatas'][0]:
                            metadata_channel = sample['metadatas'][0].get('channel', '')
                            if metadata_channel == channel_name:
                                return collection
                    except:
                        continue
            
            return None
        except Exception as e:
            print(f"âŒ ì»¬ë ‰ì…˜ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return None
    
    def _generate_hyde_document(self, query: str, channel_name: str) -> Optional[str]:
        """HyDE ë¬¸ì„œ ìƒì„± (100 í† í°ìœ¼ë¡œ ë‹¨ì¶•)"""
        try:
            prompt = f"""ë‹¹ì‹ ì€ {channel_name} ì±„ë„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•œ ì™„ë²½í•œ ë‹µë³€ì´ ë‹´ê¸´ 100í† í° ë‚´ì™¸ì˜ ê°€ìƒ ë¬¸ì„œë¥¼ ì‘ì„±í•˜ì„¸ìš”.

ì§ˆë¬¸: {query}

ì´ ì±„ë„ì˜ ê´€ì ì—ì„œ êµ¬ì²´ì ì¸ ìˆ˜ì¹˜, ì§€ì—­ëª…, ì „ëµì´ í¬í•¨ëœ ë‹µë³€ì„ ì‘ì„±í•´ì£¼ì„¸ìš”."""

            start_time = time.time()
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": f"ë‹¹ì‹ ì€ {channel_name} ì±„ë„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=100,  # 150 â†’ 100ìœ¼ë¡œ ë‹¨ì¶•
                temperature=0.7
            )
            
            hyde_doc = response.choices[0].message.content.strip()
            generation_time = (time.time() - start_time) * 1000
            
            print(f"ğŸ¯ HyDE ìƒì„± ì™„ë£Œ ({generation_time:.1f}ms, 100tok): {hyde_doc[:50]}...")
            return hyde_doc
            
        except Exception as e:
            print(f"âš ï¸ HyDE ìƒì„± ì‹¤íŒ¨: {e}")
            return None
    
    def _rewrite_query(self, query: str, channel_name: str, context: str = "") -> Optional[str]:
        """Query Rewriting - ì±„ë„ íŠ¹í™” í‚¤ì›Œë“œ ì‚½ì… (40 í† í°ìœ¼ë¡œ ë‹¨ì¶•)"""
        try:
            prompt = f"""ë‹¹ì‹ ì€ {channel_name} ì±„ë„ ì „ë¬¸ ê²€ìƒ‰ ìµœì í™” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ì´ ì±„ë„ì˜ ì»¨í…ì¸ ì—ì„œ ê²€ìƒ‰í•˜ê¸° ì‰¬ìš´ í˜•íƒœë¡œ ì¬ì‘ì„±í•˜ì„¸ìš”.

ì›ë³¸ ì§ˆë¬¸: {query}
ì±„ë„ ì»¨í…ìŠ¤íŠ¸: {context[:150]}

{channel_name} ì±„ë„ì˜ ì˜ìƒì—ì„œ ì°¾ì„ ìˆ˜ ìˆëŠ” í•µì‹¬ í‚¤ì›Œë“œì™€ ê°œë…ì„ í¬í•¨í•œ ê²€ìƒ‰ ì¿¼ë¦¬ë¡œ ì¬ì‘ì„±í•˜ì„¸ìš”.
**40í† í° ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.**"""

            start_time = time.time()
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": f"ë‹¹ì‹ ì€ {channel_name} ì±„ë„ ì „ë¬¸ ê²€ìƒ‰ ì§ˆì˜ ìµœì í™” ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=40,  # 60 â†’ 40ìœ¼ë¡œ ë‹¨ì¶•
                temperature=0.3
            )
            
            rewritten = response.choices[0].message.content.strip()
            generation_time = (time.time() - start_time) * 1000
            
            print(f"ğŸ”„ Query Rewrite ì™„ë£Œ ({generation_time:.1f}ms, 40tok): {rewritten}")
            return rewritten
            
        except Exception as e:
            print(f"âš ï¸ Query Rewriting ì‹¤íŒ¨: {e}")
            return None

    def _generate_fusion_queries(self, query: str, channel_name: str, num_queries: int = 4) -> List[str]:
        """RAG-Fusionìš© ë‹¤ì¤‘ ë³€í˜• ì¿¼ë¦¬ ìƒì„± (3-5ê°œ)"""
        try:
            prompt = f"""ë‹¹ì‹ ì€ {channel_name} ì±„ë„ ì „ë¬¸ ê²€ìƒ‰ ì „ëµê°€ì…ë‹ˆë‹¤.
ì£¼ì–´ì§„ ì§ˆë¬¸ì˜ ë‹¤ì–‘í•œ ì¸¡ë©´ì„ íƒìƒ‰í•˜ê¸° ìœ„í•´ {num_queries}ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ ë³€í˜• ì§ˆë¬¸ì„ ìƒì„±í•˜ì„¸ìš”.

ì›ë³¸ ì§ˆë¬¸: {query}

**ìƒì„± ê·œì¹™:**
1. ê°™ì€ ì˜ë„ë¥¼ ë‹¤ë¥¸ ê´€ì ì—ì„œ í‘œí˜„
2. êµ¬ì²´ì ì¸ í‚¤ì›Œë“œì™€ ì¶”ìƒì ì¸ ê°œë… í˜¼í•©
3. ì§ˆë¬¸ ê¸¸ì´ì™€ ìŠ¤íƒ€ì¼ ë‹¤ì–‘í™”
4. {channel_name} ì±„ë„ íŠ¹ì„± ë°˜ì˜

{num_queries}ê°œì˜ ë³€í˜• ì§ˆë¬¸ì„ í•œ ì¤„ì”© ë²ˆí˜¸ ì—†ì´ ì‘ì„±í•˜ì„¸ìš”:"""

            start_time = time.time()
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": f"ë‹¹ì‹ ì€ {channel_name} ì±„ë„ ì „ë¬¸ ë‹¤ê°ë„ ì§ˆë¬¸ ìƒì„± ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=150,  # 200 â†’ 150ìœ¼ë¡œ ë‹¨ì¶•
                temperature=0.8  # ì°½ì˜ì„±ì„ ìœ„í•´ ë†’ì€ temperature
            )
            
            result = response.choices[0].message.content.strip()
            generation_time = (time.time() - start_time) * 1000
            
            # ë³€í˜• ì¿¼ë¦¬ë“¤ íŒŒì‹±
            fusion_queries = []
            for line in result.split('\n'):
                line = line.strip()
                if line and not line.startswith(('1.', '2.', '3.', '4.', '5.')):
                    # ë²ˆí˜¸ ì œê±° í›„ ì •ë¦¬
                    clean_line = re.sub(r'^\d+[\.\)]\s*', '', line).strip()
                    if clean_line and clean_line != query:  # ì›ë³¸ê³¼ ë‹¤ë¥¸ ê²½ìš°ë§Œ
                        fusion_queries.append(clean_line)
            
            # ì¤‘ë³µ ì œê±° ë° ê°œìˆ˜ ì¡°ì •
            unique_queries = []
            for q in fusion_queries:
                if q not in unique_queries and len(unique_queries) < num_queries:
                    unique_queries.append(q)
            
            print(f"ğŸ¯ RAG-Fusion ì¿¼ë¦¬ ìƒì„± ì™„ë£Œ ({generation_time:.1f}ms): {len(unique_queries)}ê°œ")
            for i, fq in enumerate(unique_queries, 1):
                print(f"  {i}. {fq}")
                
            return unique_queries
            
        except Exception as e:
            print(f"âš ï¸ RAG-Fusion ì¿¼ë¦¬ ìƒì„± ì‹¤íŒ¨: {e}")
            return []
    
    def _reciprocal_rank_fusion(self, query_results: List[List[Dict]], k: int = 60) -> List[Dict]:
        """Reciprocal Rank Fusion (RRF)ìœ¼ë¡œ ë‹¤ì¤‘ ê²€ìƒ‰ ê²°ê³¼ ë³‘í•©"""
        video_scores = {}
        
        for query_idx, results in enumerate(query_results):
            for rank, doc in enumerate(results):
                video_id = doc['video_id']
                
                # RRF ì ìˆ˜ ê³„ì‚°: 1 / (k + rank)
                rrf_score = 1.0 / (k + rank + 1)
                
                if video_id not in video_scores:
                    video_scores[video_id] = {
                        'doc': doc,
                        'rrf_score': 0.0,
                        'appearances': 0,
                        'best_rank': rank + 1,
                        'query_sources': []
                    }
                
                video_scores[video_id]['rrf_score'] += rrf_score
                video_scores[video_id]['appearances'] += 1
                video_scores[video_id]['best_rank'] = min(video_scores[video_id]['best_rank'], rank + 1)
                video_scores[video_id]['query_sources'].append(query_idx)
        
        # RRF ì ìˆ˜ë¡œ ì •ë ¬
        sorted_results = sorted(
            video_scores.values(), 
            key=lambda x: x['rrf_score'], 
            reverse=True
        )
        
        # ê²°ê³¼ í¬ë§·íŒ…
        fusion_results = []
        for item in sorted_results:
            doc = item['doc'].copy()
            doc['rrf_score'] = item['rrf_score']
            doc['fusion_appearances'] = item['appearances']
            doc['best_rank'] = item['best_rank']
            doc['search_method'] = 'rag_fusion'
            fusion_results.append(doc)
        
        print(f"ğŸ”— RRF ë³‘í•© ì™„ë£Œ: {len(fusion_results)}ê°œ ë¬¸ì„œ, í‰ê·  ì¶œí˜„: {sum(item['appearances'] for item in sorted_results) / len(sorted_results):.1f}íšŒ")
        
        return fusion_results
    
    def _vector_search(self, collection, query_text: str, n_results: int = 8) -> List[Dict]:
        """ë²¡í„° ê²€ìƒ‰ ì‹¤í–‰"""
        try:
            start_time = time.time()
            results = collection.query(
                query_texts=[query_text],
                n_results=n_results,
                include=["distances", "metadatas", "documents"]
            )
            search_time = (time.time() - start_time) * 1000
            
            if not results["documents"][0]:
                return []
            
            formatted_results = []
            for i, (doc, metadata, distance) in enumerate(zip(
                results['documents'][0],
                results['metadatas'][0], 
                results['distances'][0]
            )):
                # ì˜ìƒ ë©”íƒ€ë°ì´í„° ê°•í™”
                safe_metadata = metadata if metadata else {}
                formatted_results.append({
                    'video_id': safe_metadata.get('video_id', 'unknown'),
                    'title': safe_metadata.get('title', 'Unknown Title'),
                    'content': doc,
                    'metadata': {
                        'upload_date': safe_metadata.get('upload_date', 'ë‚ ì§œ ë¯¸ìƒ'),
                        'duration': safe_metadata.get('duration', 'ì‹œê°„ ë¯¸ìƒ'),
                        'chunk_index': safe_metadata.get('chunk_index', 0),
                        'chunk_start_time': safe_metadata.get('chunk_start_time', '00:00'),
                        'channel': safe_metadata.get('channel', 'Unknown Channel'),
                        'view_count': safe_metadata.get('view_count', 'N/A'),
                        'description': safe_metadata.get('description', '')[:100] + '...' if safe_metadata.get('description') else 'N/A'
                    },
                    'distance': distance,
                    'similarity': 1 - distance,
                    'search_time_ms': search_time,
                    'relevance_category': self._get_relevance_category(1 - distance)
                })
            
            print(f"ğŸ“Š ë²¡í„° ê²€ìƒ‰ ì™„ë£Œ ({search_time:.1f}ms): {len(formatted_results)}ê°œ ë¬¸ì„œ")
            return formatted_results
            
        except Exception as e:
            print(f"âŒ ë²¡í„° ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def _should_use_rerank(self, query_type: QueryType, results_count: int) -> bool:
        """Re-rank ì‚¬ìš© ì—¬ë¶€ ê²°ì • (ì¡°ê±´ë¶€ ì‹¤í–‰)"""
        # ë³µì¡í•œ ì¿¼ë¦¬ì´ê³  ì¶©ë¶„í•œ ê²°ê³¼ê°€ ìˆì„ ë•Œë§Œ
        return (
            query_type in [QueryType.COMPLEX, QueryType.ANALYTICAL] and 
            results_count >= 5
        )
    
    def _cross_encoder_rerank(self, query: str, candidates: List[Dict], channel_name: str) -> List[Dict]:
        """Cross-Encoder ì •ë°€ Re-Ranking (ì „ë¬¸ê°€ ì¡°ì–¸: precision +12pt í–¥ìƒ)"""
        if not candidates:
            return []
        
        try:
            start_time = time.time()
            
            # ê° í›„ë³´ì— ëŒ€í•´ ê°œë³„ ì •ë°€ ì ìˆ˜ ê³„ì‚°
            scored_candidates = []
            
            for i, candidate in enumerate(candidates[:6]):  # top-6ë¡œ ì œí•œ (ì „ë¬¸ê°€ ì¡°ì–¸)
                title = candidate.get('title', 'Unknown')
                content = candidate.get('content', '')[:300]  # ë” ë§ì€ ì»¨í…ìŠ¤íŠ¸
                similarity = candidate.get('similarity', 0.0)
                
                # ì˜ìƒ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
                upload_date = candidate.get('metadata', {}).get('upload_date', 'ë‚ ì§œ ë¯¸ìƒ')
                duration = candidate.get('metadata', {}).get('duration', 'ì‹œê°„ ë¯¸ìƒ')
                chunk_time = candidate.get('metadata', {}).get('chunk_start_time', '00:00')
                
                # Cross-Encoder ìŠ¤íƒ€ì¼ ì •ë°€ ì ìˆ˜ ìš”ì²­ (ì˜ìƒ ì •ë³´ í¬í•¨)
                scoring_prompt = f"""ì§ˆë¬¸ê³¼ ì˜ìƒ ë‚´ìš©ì˜ ì—°ê´€ì„±ì„ ì •ë°€ í‰ê°€í•˜ì„¸ìš”.

## ì‚¬ìš©ì ì§ˆë¬¸
"{query}"

## ì˜ìƒ ì •ë³´
ğŸ“º **ì œëª©**: {title}
ğŸ“… **ì—…ë¡œë“œ**: {upload_date}
â±ï¸ **ê¸¸ì´**: {duration}
ğŸ“ **êµ¬ê°„**: {chunk_time}ë¶€í„° ì‹œì‘í•˜ëŠ” ë‚´ìš©
ğŸ” **ë²¡í„° ìœ ì‚¬ë„**: {similarity:.3f}

## ì˜ìƒ ë‚´ìš©
{content}

**ì˜ìƒ ì—°ê´€ì„± í‰ê°€ ê¸°ì¤€:**
1. ì§ˆë¬¸ì˜ í•µì‹¬ ì˜ë„ì™€ ì˜ìƒ ë‚´ìš©ì˜ ì§ì ‘ì  ì¼ì¹˜ë„
2. ì˜ìƒì—ì„œ ì œê³µí•˜ëŠ” êµ¬ì²´ì  ë‹µë³€ì˜ ì™„ì„±ë„
3. {channel_name} ì±„ë„ íŠ¹ì„±ê³¼ ì˜ìƒ ë§¥ë½ì˜ ì í•©ì„±
4. ì˜ìƒ ì •ë³´ì˜ ì‹ ë¢°ì„±ê³¼ ì§ˆë¬¸ í•´ê²° ëŠ¥ë ¥
5. ë‹¤ë¥¸ ì˜ìƒê³¼ì˜ ì°¨ë³„í™”ëœ ê°€ì¹˜

ì˜ìƒê³¼ ì§ˆë¬¸ì˜ ì—°ê´€ì„± ì ìˆ˜ë¥¼ 0.0~1.0 ì‚¬ì´ë¡œ í‰ê°€í•˜ì„¸ìš” (ì˜ˆ: 0.85)"""

                try:
                    response = self.client.chat.completions.create(
                        model=self.model,
                        messages=[
                            {"role": "system", "content": f"ì •ë°€í•œ ì˜ìƒ-ì§ˆë¬¸ ì—°ê´€ì„± í‰ê°€ìì…ë‹ˆë‹¤. {channel_name} ì±„ë„ì˜ ì˜ìƒ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì§ˆë¬¸ê³¼ ê°€ì¥ ê´€ë ¨ ë†’ì€ ì˜ìƒì„ ì°¾ì•„ ê°ê´€ì ìœ¼ë¡œ í‰ê°€í•˜ì„¸ìš”."},
                            {"role": "user", "content": scoring_prompt}
                        ],
                        max_tokens=8,
                        temperature=0.1  # ì¼ê´€ì„± ìš°ì„ 
                    )
                    
                    # ì ìˆ˜ ì¶”ì¶œ ë° ê²€ì¦
                    score_text = response.choices[0].message.content.strip()
                    try:
                        cross_score = float(score_text)
                        cross_score = max(0.0, min(1.0, cross_score))  # ë²”ìœ„ ì œí•œ
                    except ValueError:
                        cross_score = similarity  # fallback
                    
                except Exception as e:
                    print(f"âš ï¸ Cross-Encoder ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨ ({i+1}ë²ˆ): {e}")
                    cross_score = similarity
                
                # í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜: Cross-Encoder(75%) + Vector(25%)
                final_score = cross_score * 0.75 + similarity * 0.25
                
                candidate_scored = candidate.copy()
                candidate_scored['cross_encoder_score'] = cross_score
                candidate_scored['final_rerank_score'] = final_score
                candidate_scored['rank_score'] = final_score
                
                scored_candidates.append(candidate_scored)
                print(f"  ğŸ“Š ë¬¸ì„œ {i+1}: vec={similarity:.3f} â†’ cross={cross_score:.3f} â†’ final={final_score:.3f}")
            
            # ìµœì¢… ì ìˆ˜ë¡œ ì •ë ¬
            reranked = sorted(scored_candidates, key=lambda x: x['final_rerank_score'], reverse=True)
            
            rerank_time = (time.time() - start_time) * 1000
            print(f"ğŸ¯ Cross-Encoder Re-rank ì™„ë£Œ ({rerank_time:.1f}ms): top-{len(reranked)}ê°œ ì •ë°€ ì¬í‰ê°€")
            
            return reranked
                
        except Exception as e:
            print(f"âš ï¸ Cross-Encoder Re-ranking ì‹¤íŒ¨: {e}")
            # fallback: ë²¡í„° ìœ ì‚¬ë„ ê¸°ë°˜ ì •ë ¬
            return sorted(candidates, key=lambda x: x['similarity'], reverse=True)[:6]
    
    def _merge_and_deduplicate(self, all_results: List[List[Dict]], search_methods: List[str]) -> List[Dict]:
        """ê²€ìƒ‰ ê²°ê³¼ ë³‘í•© ë° ì¤‘ë³µ ì œê±°"""
        seen_videos = {}
        
        for results, method in zip(all_results, search_methods):
            for result in results:
                video_id = result['video_id']
                result['search_method'] = method
                
                # ë” ë†’ì€ ìœ ì‚¬ë„ì˜ ê²°ê³¼ ìœ ì§€
                if video_id not in seen_videos or result['similarity'] > seen_videos[video_id]['similarity']:
                    seen_videos[video_id] = result
        
        # ìœ ì‚¬ë„ìˆœ ì •ë ¬
        merged_results = sorted(seen_videos.values(), key=lambda x: x['similarity'], reverse=True)
        
        print(f"ğŸ”— ê²°ê³¼ ë³‘í•© ì™„ë£Œ: {len(merged_results)}ê°œ ê³ ìœ  ë¬¸ì„œ")
        return merged_results
    
    def search(self, search_query: SearchQuery, config: SearchConfig) -> SearchResult:
        """ë©”ì¸ ê²€ìƒ‰ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        start_time = time.time()
        
        print(f"ğŸ” ê²€ìƒ‰ ì‹œì‘: '{search_query.original_query}' in {search_query.channel_name}")
        
        # 1. ì±„ë„ ì»¬ë ‰ì…˜ ê°€ì ¸ì˜¤ê¸°
        collection = self._get_channel_collection(search_query.channel_name)
        if not collection:
            return SearchResult(
                query_id=search_query.query_id,
                channel_name=search_query.channel_name,
                documents=[],
                total_found=0,
                search_time_ms=0,
                hyde_used=False,
                rewrite_used=False,
                rerank_used=False
            )
        
        # 2. ì¿¼ë¦¬ ë³µì¡ë„ ë¶„ë¥˜ ë° ì¡°ê±´ë¶€ íŒŒì´í”„ë¼ì¸ ê²°ì •
        query_type = self._classify_query_complexity(search_query.original_query)
        pipeline_mode = self._select_pipeline_mode(query_type, search_query.original_query)
        print(f"ğŸ“Š ì¿¼ë¦¬ íƒ€ì…: {query_type.value} â†’ íŒŒì´í”„ë¼ì¸: {pipeline_mode}")
        
        all_results = []
        search_methods = []
        
        # 3. ì›ë³¸ ì¿¼ë¦¬ ê²€ìƒ‰ (ëª¨ë“  íŒŒì´í”„ë¼ì¸ì—ì„œ ì‹¤í–‰)
        print("ğŸ“ 1ë‹¨ê³„: ì›ë³¸ ì¿¼ë¦¬ ê²€ìƒ‰")
        original_results = self._vector_search(collection, search_query.original_query, config.max_results)
        if original_results:
            all_results.append(original_results)
            search_methods.append("original")
        
        # 4. ì¡°ê±´ë¶€ ê³ ê¸‰ ê²€ìƒ‰ ê¸°ë²• ì ìš©
        hyde_used = False
        fusion_used = False
        rewrite_used = False
        
        if pipeline_mode == "lightweight":
            # ê²½ëŸ‰ íŒŒì´í”„ë¼ì¸: ê°„ë‹¨í•œ FAQ, ì‚¬ì‹¤í˜• ì§ˆë¬¸
            print("âš¡ ê²½ëŸ‰ íŒŒì´í”„ë¼ì¸: ë²¡í„° ê²€ìƒ‰ë§Œ ìˆ˜í–‰")
            
        elif pipeline_mode == "standard":
            # í‘œì¤€ íŒŒì´í”„ë¼ì¸: HyDE + Query Rewriting
            print("ğŸ”„ í‘œì¤€ íŒŒì´í”„ë¼ì¸: HyDE + ì¿¼ë¦¬ ì¬ì‘ì„±")
            
            # HyDE ê²€ìƒ‰
            if config.enable_hyde:
                print("ğŸ¯ 2ë‹¨ê³„: HyDE ê²€ìƒ‰")
                hyde_doc = self._generate_hyde_document(search_query.original_query, search_query.channel_name)
                if hyde_doc:
                    search_query.hyde_document = hyde_doc
                    hyde_results = self._vector_search(collection, hyde_doc, config.max_results)
                    if hyde_results:
                        all_results.append(hyde_results)
                        search_methods.append("hyde")
                        hyde_used = True
            
            # Query Rewriting
            if config.enable_rewrite and all_results:
                print("ğŸ”„ 3ë‹¨ê³„: Query Rewriting ê²€ìƒ‰")
                context = all_results[0][0]['content'] if all_results[0] else ""
                rewritten_query = self._rewrite_query(search_query.original_query, search_query.channel_name, context)
                if rewritten_query and rewritten_query != search_query.original_query:
                    search_query.rewritten_query = rewritten_query
                    rewrite_results = self._vector_search(collection, rewritten_query, config.max_results)
                    if rewrite_results:
                        all_results.append(rewrite_results)
                        search_methods.append("rewritten")
                        rewrite_used = True
                        
        elif pipeline_mode == "comprehensive":
            # ì¢…í•© íŒŒì´í”„ë¼ì¸: ëª¨ë“  ê¸°ë²• í™œìš©
            print("ğŸš€ ì¢…í•© íŒŒì´í”„ë¼ì¸: ì „ì²´ ìŠ¤íƒ í™œìš©")
            
            # HyDE ê²€ìƒ‰
            if config.enable_hyde:
                print("ğŸ¯ 2ë‹¨ê³„: HyDE ê²€ìƒ‰")
                hyde_doc = self._generate_hyde_document(search_query.original_query, search_query.channel_name)
                if hyde_doc:
                    search_query.hyde_document = hyde_doc
                    hyde_results = self._vector_search(collection, hyde_doc, config.max_results)
                    if hyde_results:
                        all_results.append(hyde_results)
                        search_methods.append("hyde")
                        hyde_used = True
            
            # RAG-Fusion ê²€ìƒ‰ (ë³µì¡í•œ ì§ˆë¬¸ì—ë§Œ)
            if config.enable_rag_fusion:
                print(f"ğŸ¯ 3ë‹¨ê³„: RAG-Fusion ë‹¤ì¤‘ ì¿¼ë¦¬ ê²€ìƒ‰ ({config.rag_fusion_queries}ê°œ)")
                fusion_queries = self._generate_fusion_queries(
                    search_query.original_query, 
                    search_query.channel_name, 
                    config.rag_fusion_queries
                )
                
                if fusion_queries:
                    fusion_results_list = []
                    for i, fq in enumerate(fusion_queries):
                        print(f"  ê²€ìƒ‰ ì¤‘: {fq[:50]}...")
                        fq_results = self._vector_search(collection, fq, config.max_results)
                        if fq_results:
                            fusion_results_list.append(fq_results)
                    
                    if fusion_results_list:
                        # RRFë¡œ ë³‘í•©
                        rrf_results = self._reciprocal_rank_fusion(fusion_results_list)
                        if rrf_results:
                            all_results.append(rrf_results)
                            search_methods.append("rag_fusion")
                            fusion_used = True
            
            # Query Rewriting
            if config.enable_rewrite and all_results:
                print("ğŸ”„ 4ë‹¨ê³„: Query Rewriting ê²€ìƒ‰")
                context = all_results[0][0]['content'] if all_results[0] else ""
                rewritten_query = self._rewrite_query(search_query.original_query, search_query.channel_name, context)
                if rewritten_query and rewritten_query != search_query.original_query:
                    search_query.rewritten_query = rewritten_query
                    rewrite_results = self._vector_search(collection, rewritten_query, config.max_results)
                    if rewrite_results:
                        all_results.append(rewrite_results)
                        search_methods.append("rewritten")
                        rewrite_used = True
        
        # 7. ê²°ê³¼ ë³‘í•© ë° ì¤‘ë³µ ì œê±°
        if not all_results:
            search_time_ms = (time.time() - start_time) * 1000
            return SearchResult(
                query_id=search_query.query_id,
                channel_name=search_query.channel_name,
                documents=[],
                total_found=0,
                search_time_ms=search_time_ms,
                hyde_used=hyde_used,
                fusion_used=fusion_used,
                rewrite_used=rewrite_used,
                rerank_used=False
            )
        
        merged_results = self._merge_and_deduplicate(all_results, search_methods)
        
        # 8. 2ì¸µ ìœ ì‚¬ë„ í•„í„°ë§ - ì „ë¬¸ê°€ ì¡°ì–¸ ë°˜ì˜
        # 1ì°¨ í•„í„°: recall ìµœì í™” (ë‚®ì€ threshold)
        first_filter = [r for r in merged_results if r['similarity'] > config.similarity_threshold]
        print(f"ğŸ” 1ì°¨ í•„í„° (recall={config.similarity_threshold}): {len(merged_results)} â†’ {len(first_filter)}ê°œ")
        
        # 9. ì¡°ê±´ë¶€ Cross-Encoder Re-ranking (ì „ë¬¸ê°€ ì¡°ì–¸ ë°˜ì˜)
        rerank_used = False
        final_results = first_filter
        
        if config.enable_rerank and self._should_use_rerank(query_type, len(first_filter)):
            print(f"ğŸ¯ 5ë‹¨ê³„: Cross-Encoder Re-ranking (top-{config.rerank_top_k})")
            # Re-rankingìš© í›„ë³´: 12ê°œì—ì„œ top-6 ì„ ë³„ (ì „ë¬¸ê°€ ì¡°ì–¸: precision +12pt)
            rerank_candidates = first_filter[:12]  # ë” ë§ì€ í›„ë³´ì—ì„œ ì„ ë³„
            reranked_results = self._cross_encoder_rerank(search_query.original_query, rerank_candidates, search_query.channel_name)
            if reranked_results:
                final_results = reranked_results[:config.rerank_top_k]  # top-kë¡œ ì œí•œ
                rerank_used = True
        
        # 10. 2ì°¨ í•„í„°ë§: ì •ë°€ë„ ìµœì í™” (ë†’ì€ threshold) - Re-rank í›„ ì ìš©
        if rerank_used:
            # Re-rankëœ ê²°ê³¼ëŠ” ì´ë¯¸ í’ˆì§ˆì´ ê²€ì¦ë˜ì—ˆìœ¼ë¯€ë¡œ precision_threshold ì ìš© ì•ˆ í•¨
            precision_filtered = final_results
        else:
            # Re-rank ì—†ëŠ” ê²½ìš°ë§Œ precision threshold ì ìš©
            precision_filtered = [r for r in final_results if r['similarity'] > config.precision_threshold]
            print(f"ğŸ¯ 2ì°¨ í•„í„° (precision={config.precision_threshold}): {len(final_results)} â†’ {len(precision_filtered)}ê°œ")
            final_results = precision_filtered
        
        # 11. ìµœì¢… ê²°ê³¼ ê°œìˆ˜ ì œí•œ (ì „ë¬¸ê°€ ì¡°ì–¸: top-6 ìµœì )
        display_limit = min(6, config.rerank_top_k if rerank_used else config.max_results)
        final_documents = final_results[:display_limit]
        
        # 12. SearchDocument ê°ì²´ë¡œ ë³€í™˜
        search_documents = []
        for doc in final_documents:
            search_documents.append(SearchDocument(
                video_id=doc['video_id'],
                title=doc['title'],
                content=doc['content'],
                similarity=doc['similarity'],
                metadata=doc['metadata'],
                search_method=doc['search_method'],
                rank_score=doc.get('rank_score')
            ))
        
        search_time_ms = (time.time() - start_time) * 1000
        
        # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
        used_methods = []
        if hyde_used: used_methods.append("HyDE")
        if fusion_used: used_methods.append("RAG-Fusion")
        if rewrite_used: used_methods.append("Rewrite")
        if rerank_used: used_methods.append("Re-rank")
        
        methods_str = " + ".join(used_methods) if used_methods else "ê¸°ë³¸"
        print(f"âœ… ê²€ìƒ‰ ì™„ë£Œ ({search_time_ms:.1f}ms): {len(search_documents)}ê°œ ë¬¸ì„œ [{methods_str}]")
        
        return SearchResult(
            query_id=search_query.query_id,
            channel_name=search_query.channel_name,
            documents=search_documents,
            total_found=len(merged_results),
            search_time_ms=search_time_ms,
            hyde_used=hyde_used,
            fusion_used=fusion_used,
            rewrite_used=rewrite_used,
            rerank_used=rerank_used
        ) 