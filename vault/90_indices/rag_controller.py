#!/usr/bin/env python3
"""
RAG Controller - Search-First & Prompt-Light í†µí•© ì œì–´
íŒŒì´í”„ë¼ì¸ í†µí•© + ì¡°ê±´ë¶€ ì‹¤í–‰ + ìºì‹± + ì„±ëŠ¥ ìµœì í™”

ì¡°ì–¸ ê¸°ë°˜ ì•„í‚¤í…ì²˜:
- ê²€ìƒ‰ í’ˆì§ˆì„ 'í•˜ë“œ'í•˜ê²Œ ì˜¬ë¦¬ê³ , í”„ë¡¬í”„íŠ¸ëŠ” 'ì‹¬í”Œ+ê²€ì¦'ìœ¼ë¡œ ìœ ì§€
- LLM í˜¸ì¶œÂ·ë¹„ìš©ì„ ìºì‹±ê³¼ ì¡°ê±´ë¶€ íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ì ˆê°
- 800ms â†’ < 500ms ëª©í‘œ
"""

import os
import time
import uuid
from typing import Optional, Dict, Any, List
from pathlib import Path
from dotenv import load_dotenv
from openai import OpenAI

from schemas import (
    SearchQuery, SearchConfig, SearchResult,
    AnswerRequest, AnswerResponse, AnswerConfig, 
    RAGResponse, QueryType
)
from search_pipeline import SearchPipeline
from answer_pipeline import AnswerPipeline
from semantic_cache import SemanticCache, CachedLLMClient

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

class RAGController:
    """Search-First & Prompt-Light RAG ì‹œìŠ¤í…œ í†µí•© ì»¨íŠ¸ë¡¤ëŸ¬"""
    
    def __init__(self, chroma_path: Path, model: str = "deepseek-chat", enable_cache: bool = True):
        """ì´ˆê¸°í™”"""
        self.model = model
        self.chroma_path = chroma_path
        
        print(f"ğŸš€ RAG Controller ì´ˆê¸°í™” ì‹œì‘ (ëª¨ë¸: {model})")
        
        # ìºì‹œ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        self.cache = None
        self.cached_client = None
        
        if enable_cache:
            try:
                cache_dir = chroma_path.parent / "cache"
                self.cache = SemanticCache(cache_dir)
                
                # DeepSeek í´ë¼ì´ì–¸íŠ¸
                api_key = os.getenv('DEEPSEEK_API_KEY')
                if api_key:
                    raw_client = OpenAI(api_key=api_key, base_url="https://api.deepseek.com/v1")
                    self.cached_client = CachedLLMClient(raw_client, self.cache)
                    print("âœ… ìºì‹œ ì‹œìŠ¤í…œ í™œì„±í™”")
                else:
                    print("âš ï¸ DEEPSEEK_API_KEY ì—†ìŒ, ìºì‹œ ë¹„í™œì„±í™”")
            except Exception as e:
                print(f"âš ï¸ ìºì‹œ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        
        # ê²€ìƒ‰ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
        try:
            self.search_pipeline = SearchPipeline(chroma_path, model)
            print("âœ… Search Pipeline ë¡œë“œë¨")
        except Exception as e:
            raise ValueError(f"âŒ Search Pipeline ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        
        # ë‹µë³€ íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”  
        try:
            prompts_dir = chroma_path.parent / "prompts"
            self.answer_pipeline = AnswerPipeline(model, prompts_dir)
            print("âœ… Answer Pipeline ë¡œë“œë¨")
        except Exception as e:
            raise ValueError(f"âŒ Answer Pipeline ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        
        # ê¸°ë³¸ ì„¤ì •
        self.default_search_config = SearchConfig(
            max_results=5,
            enable_hyde=True,
            enable_rewrite=True,
            enable_rerank=True,
            rerank_threshold=0.3,
            similarity_threshold=0.25
        )
        
        self.default_answer_config = AnswerConfig(
            enable_self_refine=True,
            enable_react=False,  # ê¸°ë³¸ì ìœ¼ë¡œ ë¹„í™œì„±í™”, í•„ìš”ì‹œì—ë§Œ
            max_tokens=800,
            temperature=0.7
        )
        
        print("ğŸ¯ RAG Controller ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _optimize_search_config(self, query: str, query_type: QueryType) -> SearchConfig:
        """ì¿¼ë¦¬ íƒ€ì…ì— ë”°ë¥¸ ê²€ìƒ‰ ì„¤ì • ìµœì í™”"""
        config = SearchConfig(**self.default_search_config.dict())
        
        # ë‹¨ìˆœí•œ ì¿¼ë¦¬ëŠ” ê²½ëŸ‰í™”
        if query_type == QueryType.SIMPLE:
            config.enable_rerank = False  # Re-rank ìƒëµ
            config.max_results = 3        # ê²°ê³¼ ìˆ˜ ì œí•œ
            print("ğŸ”§ ë‹¨ìˆœ ì¿¼ë¦¬: ê²½ëŸ‰ ê²€ìƒ‰ ëª¨ë“œ")
            
        # ë³µì¡í•œ ì¿¼ë¦¬ëŠ” ì „ì²´ íŒŒì´í”„ë¼ì¸ í™œìš©
        elif query_type == QueryType.COMPLEX:
            config.enable_rerank = True
            config.max_results = 5
            config.rerank_threshold = 0.25  # ë” ë‚®ì€ ì„ê³„ê°’
            print("ğŸ”§ ë³µì¡ ì¿¼ë¦¬: ê³ í’ˆì§ˆ ê²€ìƒ‰ ëª¨ë“œ")
            
        # ì‚¬ì‹¤ í™•ì¸ ì¿¼ë¦¬ëŠ” ì •í™•ë„ ìš°ì„ 
        elif query_type == QueryType.FACTUAL:
            config.enable_rerank = True
            config.similarity_threshold = 0.35  # ë” ë†’ì€ ì„ê³„ê°’
            print("ğŸ”§ ì‚¬ì‹¤ í™•ì¸: ì •í™•ë„ ìš°ì„  ëª¨ë“œ")
        
        return config
    
    def _optimize_answer_config(self, query: str, search_result: SearchResult) -> AnswerConfig:
        """ê²€ìƒ‰ ê²°ê³¼ì— ë”°ë¥¸ ë‹µë³€ ì„¤ì • ìµœì í™”"""
        config = AnswerConfig(**self.default_answer_config.dict())
        
        # ê²€ìƒ‰ ê²°ê³¼ê°€ ìš°ìˆ˜í•˜ë©´ Self-Refine ìƒëµ
        if search_result.documents:
            avg_similarity = sum(doc.similarity for doc in search_result.documents) / len(search_result.documents)
            
            if avg_similarity > 0.7:
                config.enable_self_refine = False
                print("ğŸ”§ ê³ í’ˆì§ˆ ê²€ìƒ‰ ê²°ê³¼: Self-Refine ìƒëµ")
            
            # ê²€ìƒ‰ ê²°ê³¼ê°€ ë¶€ì¡±í•˜ë©´ ReAct í™œì„±í™”
            if len(search_result.documents) < 3 or avg_similarity < 0.4:
                config.enable_react = True
                print("ğŸ”§ ê²€ìƒ‰ ê²°ê³¼ ë¶€ì¡±: ReAct í™œì„±í™”")
        
        return config
    
    def _should_use_fast_mode(self, query: str) -> bool:
        """ë¹ ë¥¸ ëª¨ë“œ ì‚¬ìš© ì—¬ë¶€ ê²°ì • (ì„±ëŠ¥ ìš°ì„ )"""
        # ì§§ê³  ê°„ë‹¨í•œ ì§ˆë¬¸
        if len(query) < 20:
            return True
        
        # íŠ¹ì • íŒ¨í„´ (ì •ì˜, ì„¤ëª… ë“±)
        fast_patterns = [
            r'\b(ë­|ë¬´ì—‡|ì •ì˜|ì„¤ëª…|ì˜ë¯¸)\b',
            r'\b(ì–¸ì œ|ì–´ë””|ëˆ„êµ¬|ëª‡)\b',
            r'\b(ê°„ë‹¨íˆ|ë¹ ë¥´ê²Œ|ìš”ì•½)\b'
        ]
        
        import re
        for pattern in fast_patterns:
            if re.search(pattern, query):
                return True
        
        return False
    
    def query(self, query: str, channel_name: str, 
              search_config: Optional[SearchConfig] = None,
              answer_config: Optional[AnswerConfig] = None,
              fast_mode: bool = False) -> RAGResponse:
        """ë©”ì¸ RAG ì¿¼ë¦¬ ì²˜ë¦¬"""
        start_time = time.time()
        query_id = str(uuid.uuid4())[:8]
        
        print(f"ğŸ” RAG Query ì‹œì‘: {query_id} - '{query}' in {channel_name}")
        
        # ë¹ ë¥¸ ëª¨ë“œ ìë™ íŒë‹¨
        if not fast_mode:
            fast_mode = self._should_use_fast_mode(query)
        
        if fast_mode:
            print("âš¡ ë¹ ë¥¸ ëª¨ë“œ í™œì„±í™”")
        
        try:
            # 1. ê²€ìƒ‰ ì¿¼ë¦¬ êµ¬ì„±
            search_query = SearchQuery(
                query_id=query_id,
                original_query=query,
                channel_name=channel_name
            )
            
            # 2. ê²€ìƒ‰ ì„¤ì • ìµœì í™”
            if search_config is None:
                search_config = self._optimize_search_config(query, search_query.query_type)
            
            # ë¹ ë¥¸ ëª¨ë“œì—ì„œëŠ” ê²€ìƒ‰ ë‹¨ìˆœí™”
            if fast_mode:
                search_config.enable_rerank = False
                search_config.enable_rewrite = False
                search_config.max_results = 3
            
            # 3. ê²€ìƒ‰ ì‹¤í–‰
            search_start = time.time()
            search_result = self.search_pipeline.search(search_query, search_config)
            search_time = (time.time() - search_start) * 1000
            
            if not search_result.documents:
                return RAGResponse(
                    query_id=query_id,
                    channel_name=channel_name,
                    original_query=query,
                    answer=f"{channel_name} ì±„ë„ì—ì„œ ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                    confidence=0.0,
                    total_time_ms=(time.time() - start_time) * 1000,
                    search_time_ms=search_time,
                    answer_time_ms=0,
                    documents_found=0,
                    sources_used=[]
                )
            
            # 4. ë‹µë³€ ì„¤ì • ìµœì í™”
            if answer_config is None:
                answer_config = self._optimize_answer_config(query, search_result)
            
            # ë¹ ë¥¸ ëª¨ë“œì—ì„œëŠ” ë‹µë³€ ë‹¨ìˆœí™”
            if fast_mode:
                answer_config.enable_self_refine = False
                answer_config.enable_react = False
                answer_config.max_tokens = 400
            
            # 5. ë‹µë³€ ìƒì„± ìš”ì²­ êµ¬ì„±
            answer_request = AnswerRequest(
                query_id=query_id,
                original_query=query,
                search_result=search_result,
                config=answer_config
            )
            
            # 6. ë‹µë³€ ìƒì„± ì‹¤í–‰
            answer_start = time.time()
            answer_response = self.answer_pipeline.generate_answer(answer_request)
            answer_time = (time.time() - answer_start) * 1000
            
            total_time = (time.time() - start_time) * 1000
            
            # 7. ìµœì¢… ì‘ë‹µ êµ¬ì„±
            rag_response = RAGResponse(
                query_id=query_id,
                channel_name=channel_name,
                original_query=query,
                answer=answer_response.answer,
                confidence=answer_response.confidence,
                total_time_ms=total_time,
                search_time_ms=search_time,
                answer_time_ms=answer_time,
                documents_found=len(search_result.documents),
                sources_used=answer_response.sources_used,
                search_quality={
                    "hyde_used": search_result.hyde_used,
                    "rewrite_used": search_result.rewrite_used,
                    "rerank_used": search_result.rerank_used,
                    "avg_similarity": sum(doc.similarity for doc in search_result.documents) / len(search_result.documents) if search_result.documents else 0.0
                },
                debug_info={
                    "query_type": search_query.query_type.value,
                    "fast_mode": fast_mode,
                    "self_refined": answer_response.self_refined,
                    "react_steps": answer_response.react_steps,
                    "token_usage": answer_response.token_usage,
                    "cache_used": self.cache is not None
                }
            )
            
            # ì„±ëŠ¥ ëª©í‘œ ì²´í¬
            if total_time > 500:
                print(f"âš ï¸ ì„±ëŠ¥ ëª©í‘œ ì´ˆê³¼: {total_time:.1f}ms > 500ms")
            else:
                print(f"âœ… ì„±ëŠ¥ ëª©í‘œ ë‹¬ì„±: {total_time:.1f}ms < 500ms")
            
            print(f"ğŸ¯ RAG Query ì™„ë£Œ: {query_id} ({total_time:.1f}ms)")
            return rag_response
            
        except Exception as e:
            total_time = (time.time() - start_time) * 1000
            print(f"âŒ RAG Query ì‹¤íŒ¨: {query_id} - {e}")
            
            return RAGResponse(
                query_id=query_id,
                channel_name=channel_name,
                original_query=query,
                answer=f"ì£„ì†¡í•©ë‹ˆë‹¤. ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}",
                confidence=0.0,
                total_time_ms=total_time,
                search_time_ms=0,
                answer_time_ms=0,
                documents_found=0,
                sources_used=[],
                debug_info={"error": str(e)}
            )
    
    def get_available_channels(self) -> List[Dict[str, Any]]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ì±„ë„ ëª©ë¡ ë°˜í™˜"""
        return self.search_pipeline.chroma_client.list_collections()
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """ìºì‹œ í†µê³„ ì¡°íšŒ"""
        if self.cache:
            return self.cache.get_stats()
        return {"cache_enabled": False}
    
    def cleanup_cache(self) -> int:
        """ë§Œë£Œëœ ìºì‹œ ì •ë¦¬"""
        if self.cache:
            return self.cache.cleanup_expired()
        return 0
    
    def clear_cache(self) -> bool:
        """ì „ì²´ ìºì‹œ ì‚­ì œ"""
        if self.cache:
            return self.cache.clear()
        return False
    
    def health_check(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ìƒíƒœ ì²´í¬"""
        health_status = {
            "status": "healthy",
            "components": {
                "search_pipeline": True,
                "answer_pipeline": True,
                "cache": self.cache is not None,
                "chroma_db": False
            },
            "performance": {
                "last_query_time": None,
                "cache_hit_rate": 0.0
            }
        }
        
        # ChromaDB ì—°ê²° ì²´í¬
        try:
            collections = self.search_pipeline.chroma_client.list_collections()
            health_status["components"]["chroma_db"] = True
            health_status["chroma_collections"] = len(collections)
        except Exception:
            health_status["components"]["chroma_db"] = False
            health_status["status"] = "degraded"
        
        # ìºì‹œ ìƒíƒœ ì²´í¬
        if self.cache:
            cache_stats = self.cache.get_stats()
            health_status["performance"]["cache_hit_rate"] = cache_stats["hit_rate"]
        
        return health_status 