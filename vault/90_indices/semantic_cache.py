#!/usr/bin/env python3
"""
L2 Semantic Cache System - LLM í˜¸ì¶œ 40%â†’65% ì ˆê° (ì „ë¬¸ê°€ ì¡°ì–¸ ë°˜ì˜)
í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê¸°ë°˜ ì§ˆë¬¸ ë§¤ì¹­ìœ¼ë¡œ ì§€ì—° -120ms ë‹¬ì„±

L2 ìºì‹œ êµ¬ì¡°:
- L1: (model, temperature, exact_prompt_hash) - ì •í™• ë§¤ì¹­
- L2: (query_normalized_hash, fuzzy_matching) - í…ìŠ¤íŠ¸ ìœ ì‚¬ ë§¤ì¹­
- ì •ê·œí™”ëœ ì§ˆë¬¸ë“¤ì„ ìë™ìœ¼ë¡œ ìºì‹œ ì¬ì‚¬ìš©
- ë‹¨ì–´ ê¸°ë°˜ ë¹ ë¥¸ ìœ ì‚¬ì„± ê²€ìƒ‰
"""

import os
import hashlib
import json
import time
import pickle
import re
from typing import Dict, Optional, Any, List, Union, Tuple
from pathlib import Path
from datetime import datetime, timedelta
from dataclasses import dataclass
import sqlite3
from threading import Lock
from schemas import CacheKey, CacheEntry

@dataclass
class CacheStats:
    """L2 ìºì‹œ í†µê³„ (ì „ë¬¸ê°€ ì¡°ì–¸ ì¶”ì )"""
    total_requests: int = 0
    l1_hits: int = 0      # ì •í™• ë§¤ì¹­ íˆíŠ¸
    l2_hits: int = 0      # ìœ ì‚¬ ì§ˆë¬¸ ë§¤ì¹­ íˆíŠ¸  
    cache_misses: int = 0
    avg_latency_saved_ms: float = 0.0  # ì ˆì•½ëœ í‰ê·  ì§€ì—°ì‹œê°„
    
    @property
    def total_hits(self) -> int:
        return self.l1_hits + self.l2_hits
    
    @property
    def hit_rate(self) -> float:
        return self.total_hits / self.total_requests if self.total_requests > 0 else 0.0
    
    @property
    def l2_effectiveness(self) -> float:
        """L2 ìºì‹œ íš¨ê³¼ì„± (L2 íˆíŠ¸ / ì „ì²´ íˆíŠ¸)"""
        return self.l2_hits / self.total_hits if self.total_hits > 0 else 0.0

class SemanticCache:
    """L2 Semantic Cache ì‹œìŠ¤í…œ (ì „ë¬¸ê°€ ì¡°ì–¸ ë°˜ì˜)"""
    
    def __init__(self, cache_dir: Path = None, ttl_seconds: int = 604800, 
                 similarity_threshold: float = 0.7, enable_l2: bool = True):  # 7ì¼
        """ì´ˆê¸°í™” - L2 í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ìºì‹œ ì§€ì›"""
        self.cache_dir = cache_dir or Path(__file__).parent / "cache"
        self.cache_dir.mkdir(exist_ok=True)
        self.ttl_seconds = ttl_seconds
        self.similarity_threshold = similarity_threshold  # L2 ìºì‹œìš© í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ì„ê³„ê°’
        self.enable_l2 = enable_l2
        self.stats = CacheStats()
        self._lock = Lock()
        
        # SQLite DB ì´ˆê¸°í™” (ë©”íƒ€ë°ì´í„° ì €ì¥ìš©)
        self.db_path = self.cache_dir / "cache_meta.db"
        self._init_db()
        
        print(f"ğŸ’¾ Semantic Cache ì´ˆê¸°í™”: {self.cache_dir}")
        print(f"â° TTL: {ttl_seconds}ì´ˆ ({ttl_seconds//86400}ì¼)")
        print(f"ğŸ”„ L2 í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ìºì‹œ: {'í™œì„±í™”' if self.enable_l2 else 'ë¹„í™œì„±í™”'} (ì„ê³„ê°’: {similarity_threshold})")
    
    def _init_db(self):
        """SQLite DB ì´ˆê¸°í™” - L2 ì„ë² ë”© í…Œì´ë¸” ì¶”ê°€"""
        with sqlite3.connect(self.db_path) as conn:
            # L1 ìºì‹œ í…Œì´ë¸” (ê¸°ì¡´)
            conn.execute("""
                CREATE TABLE IF NOT EXISTS cache_entries (
                    key_hash TEXT PRIMARY KEY,
                    model TEXT,
                    temperature REAL,
                    prompt_hash TEXT,
                    query_hash TEXT,
                    created_at TIMESTAMP,
                    ttl_seconds INTEGER,
                    hit_count INTEGER DEFAULT 0,
                    data_file TEXT,
                    data_size INTEGER
                )
            """)
            
            # L2 ìºì‹œ í…Œì´ë¸” (ìƒˆë¡œ ì¶”ê°€ - í…ìŠ¤íŠ¸ ê¸°ë°˜ ìœ ì‚¬ ì§ˆë¬¸ ë§¤ì¹­)
            conn.execute("""
                CREATE TABLE IF NOT EXISTS l2_query_patterns (
                    pattern_hash TEXT PRIMARY KEY,
                    original_query TEXT,
                    normalized_query TEXT,
                    query_keywords TEXT,
                    model TEXT,
                    temperature REAL,
                    cache_key_hash TEXT,
                    created_at TIMESTAMP,
                    similarity_hits INTEGER DEFAULT 0,
                    FOREIGN KEY (cache_key_hash) REFERENCES cache_entries (key_hash)
                )
            """)
            
            # ì¸ë±ìŠ¤ ìƒì„±
            conn.execute("""
                CREATE INDEX IF NOT EXISTS idx_created_ttl 
                ON cache_entries (created_at, ttl_seconds)
            """)
            
            conn.execute("""
                CREATE INDEX IF NOT EXISTS idx_model_temp 
                ON cache_entries (model, temperature)
            """)
            
            conn.execute("""
                CREATE INDEX IF NOT EXISTS idx_l2_model_temp 
                ON l2_query_patterns (model, temperature)
            """)
            
            conn.execute("""
                CREATE INDEX IF NOT EXISTS idx_l2_keywords 
                ON l2_query_patterns (query_keywords)
            """)
            
            conn.execute("""
                CREATE INDEX IF NOT EXISTS idx_l2_created 
                ON l2_query_patterns (created_at)
            """)
    
    def _generate_cache_key(self, model: str, temperature: float, prompt: str, query: str = "") -> CacheKey:
        """ìºì‹œ í‚¤ ìƒì„±"""
        prompt_hash = hashlib.sha256(prompt.encode('utf-8')).hexdigest()[:16]
        query_hash = hashlib.sha256(query.encode('utf-8')).hexdigest()[:16] if query else ""
        
        return CacheKey(
            model=model,
            temperature=temperature,
            prompt_hash=prompt_hash,
            query_hash=query_hash
        )
    
    def _get_key_hash(self, cache_key: CacheKey) -> str:
        """ìºì‹œ í‚¤ë¥¼ ë¬¸ìì—´ í•´ì‹œë¡œ ë³€í™˜"""
        key_str = f"{cache_key.model}:{cache_key.temperature}:{cache_key.prompt_hash}:{cache_key.query_hash}"
        return hashlib.sha256(key_str.encode('utf-8')).hexdigest()[:32]
    
    def _normalize_query(self, query: str) -> str:
        """ì§ˆë¬¸ ì •ê·œí™” (L2 ìºì‹œìš©)"""
        # ì†Œë¬¸ì ë³€í™˜
        normalized = query.lower()
        
        # íŠ¹ìˆ˜ë¬¸ì ì œê±° (í•œê¸€, ì˜ë¬¸, ìˆ«ìë§Œ ìœ ì§€)
        normalized = re.sub(r'[^\w\sê°€-í£]', ' ', normalized)
        
        # ì—¬ëŸ¬ ê³µë°±ì„ í•˜ë‚˜ë¡œ
        normalized = re.sub(r'\s+', ' ', normalized)
        
        # ì–‘ë ê³µë°± ì œê±°
        normalized = normalized.strip()
        
        return normalized
    
    def _get_query_keywords(self, query: str) -> List[str]:
        """í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ (L2 ìºì‹œìš©)"""
        normalized = self._normalize_query(query)
        
        # ë¶ˆìš©ì–´ ë¦¬ìŠ¤íŠ¸ (í•œêµ­ì–´)
        stopwords = {'ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì˜', 'ì—', 'ì—ì„œ', 'ë¡œ', 'ì™€', 'ê³¼', 'í•˜ê³ ', 
                    'ê·¸ë¦¬ê³ ', 'ë˜í•œ', 'í•˜ì§€ë§Œ', 'ê·¸ëŸ¬ë‚˜', 'ê·¸ëŸ°ë°', 'ë”°ë¼ì„œ', 'ê·¸ë˜ì„œ', 'ê·¸ëŸ¬ë¯€ë¡œ',
                    'ë¬´ì—‡', 'ì–´ë–¤', 'ì–´ë–»ê²Œ', 'ì™œ', 'ì–¸ì œ', 'ì–´ë””', 'ëˆ„ê°€', 'ëª‡', 'ì–¼ë§ˆ'}
        
        # ë‹¨ì–´ ë¶„ë¦¬ ë° ë¶ˆìš©ì–´ ì œê±°
        words = [word for word in normalized.split() if len(word) > 1 and word not in stopwords]
        
        # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
        keywords = sorted(list(set(words)))
        
        return keywords[:10]  # ìµœëŒ€ 10ê°œ í‚¤ì›Œë“œ
    
    def _calculate_text_similarity(self, query1: str, query2: str) -> float:
        """í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê³„ì‚° (Jaccard similarity + keyword overlap)"""
        keywords1 = set(self._get_query_keywords(query1))
        keywords2 = set(self._get_query_keywords(query2))
        
        if not keywords1 and not keywords2:
            return 1.0
        if not keywords1 or not keywords2:
            return 0.0
        
        # Jaccard similarity
        intersection = keywords1.intersection(keywords2)
        union = keywords1.union(keywords2)
        
        jaccard = len(intersection) / len(union) if union else 0.0
        
        # ê¸¸ì´ ìœ ì‚¬ì„± (ë³´ë„ˆìŠ¤)
        len1, len2 = len(query1), len(query2)
        length_similarity = 1.0 - abs(len1 - len2) / max(len1, len2, 1)
        
        # ìµœì¢… ìœ ì‚¬ë„ (Jaccard 80% + ê¸¸ì´ ìœ ì‚¬ì„± 20%)
        final_similarity = jaccard * 0.8 + length_similarity * 0.2
        
        return final_similarity
    
    def _is_expired(self, created_at: datetime, ttl_seconds: int) -> bool:
        """TTL ë§Œë£Œ í™•ì¸"""
        expiry_time = created_at + timedelta(seconds=ttl_seconds)
        return datetime.now() > expiry_time
    
    def _save_data_file(self, key_hash: str, data: Any) -> str:
        """ë°ì´í„° íŒŒì¼ ì €ì¥"""
        data_file = f"{key_hash}.pkl"
        file_path = self.cache_dir / data_file
        
        with open(file_path, 'wb') as f:
            pickle.dump(data, f)
        
        return data_file
    
    def _load_data_file(self, data_file: str) -> Any:
        """ë°ì´í„° íŒŒì¼ ë¡œë“œ"""
        file_path = self.cache_dir / data_file
        
        if not file_path.exists():
            return None
        
        try:
            with open(file_path, 'rb') as f:
                return pickle.load(f)
        except Exception as e:
            print(f"âš ï¸ ìºì‹œ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    def get(self, model: str, temperature: float, prompt: str, query: str = "") -> Optional[Any]:
        """L1 + L2 ìºì‹œì—ì„œ ë°ì´í„° ì¡°íšŒ"""
        with self._lock:
            self.stats.total_requests += 1
            
            # L1 ìºì‹œ ì¡°íšŒ (ì •í™• ë§¤ì¹­)
            cache_key = self._generate_cache_key(model, temperature, prompt, query)
            key_hash = self._get_key_hash(cache_key)
            
            try:
                with sqlite3.connect(self.db_path) as conn:
                    cursor = conn.execute("""
                        SELECT created_at, ttl_seconds, hit_count, data_file
                        FROM cache_entries 
                        WHERE key_hash = ?
                    """, (key_hash,))
                    
                    row = cursor.fetchone()
                    if row:
                        created_at_str, ttl_seconds, hit_count, data_file = row
                        created_at = datetime.fromisoformat(created_at_str)
                        
                        # TTL ë§Œë£Œ í™•ì¸
                        if not self._is_expired(created_at, ttl_seconds):
                            # ë°ì´í„° ë¡œë“œ
                            data = self._load_data_file(data_file)
                            if data is not None:
                                # íˆíŠ¸ ì¹´ìš´íŠ¸ ì¦ê°€
                                conn.execute("""
                                    UPDATE cache_entries 
                                    SET hit_count = ? 
                                    WHERE key_hash = ?
                                """, (hit_count + 1, key_hash))
                                
                                self.stats.l1_hits += 1
                                print(f"âœ… L1 ìºì‹œ íˆíŠ¸: {key_hash[:8]}... (hit_count: {hit_count + 1})")
                                return data
                        else:
                            self._delete_entry(key_hash, data_file)
                    
                    # L2 ìºì‹œ ì¡°íšŒ (ìœ ì‚¬ ì§ˆë¬¸ ë§¤ì¹­)
                    if self.enable_l2 and query:
                        l2_result = self._find_similar_cached_query(model, temperature, query, conn)
                        if l2_result:
                            self.stats.l2_hits += 1
                            print(f"ğŸ”„ L2 ìºì‹œ íˆíŠ¸: ìœ ì‚¬ë„ {l2_result['similarity']:.3f}")
                            return l2_result['data']
                    
                    self.stats.cache_misses += 1
                    return None
                    
            except Exception as e:
                print(f"âš ï¸ ìºì‹œ ì¡°íšŒ ì‹¤íŒ¨: {e}")
                self.stats.cache_misses += 1
                return None
    
    def _find_similar_cached_query(self, model: str, temperature: float, query: str, conn) -> Optional[Dict]:
        """L2 ìºì‹œì—ì„œ ìœ ì‚¬í•œ ì§ˆë¬¸ ì°¾ê¸°"""
        try:
            # í˜„ì¬ ì§ˆë¬¸ì˜ í‚¤ì›Œë“œ
            query_keywords = set(self._get_query_keywords(query))
            
            # ê°™ì€ ëª¨ë¸/ì˜¨ë„ì˜ L2 íŒ¨í„´ ì¡°íšŒ
            cursor = conn.execute("""
                SELECT l2.original_query, l2.cache_key_hash, l2.similarity_hits,
                       ce.created_at, ce.ttl_seconds, ce.data_file
                FROM l2_query_patterns l2
                JOIN cache_entries ce ON l2.cache_key_hash = ce.key_hash
                WHERE l2.model = ? AND l2.temperature = ?
                ORDER BY l2.similarity_hits DESC
                LIMIT 20
            """, (model, temperature))
            
            best_match = None
            best_similarity = 0.0
            
            for row in cursor.fetchall():
                cached_query, cache_key_hash, similarity_hits, created_at_str, ttl_seconds, data_file = row
                
                # TTL í™•ì¸
                created_at = datetime.fromisoformat(created_at_str)
                if self._is_expired(created_at, ttl_seconds):
                    continue
                
                # ìœ ì‚¬ë„ ê³„ì‚°
                similarity = self._calculate_text_similarity(query, cached_query)
                
                if similarity >= self.similarity_threshold and similarity > best_similarity:
                    # ë°ì´í„° ë¡œë“œ ì‹œë„
                    data = self._load_data_file(data_file)
                    if data is not None:
                        best_match = {
                            'data': data,
                            'similarity': similarity,
                            'original_query': cached_query,
                            'cache_key_hash': cache_key_hash
                        }
                        best_similarity = similarity
            
            # ê°€ì¥ ì¢‹ì€ ë§¤ì¹­ ì—…ë°ì´íŠ¸
            if best_match:
                conn.execute("""
                    UPDATE l2_query_patterns 
                    SET similarity_hits = similarity_hits + 1 
                    WHERE cache_key_hash = ?
                """, (best_match['cache_key_hash'],))
            
            return best_match
            
        except Exception as e:
            print(f"âš ï¸ L2 ìºì‹œ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return None
    
    def set(self, model: str, temperature: float, prompt: str, data: Any, query: str = "", ttl_seconds: int = None) -> bool:
        """L1 + L2 ìºì‹œì— ë°ì´í„° ì €ì¥"""
        with self._lock:
            cache_key = self._generate_cache_key(model, temperature, prompt, query)
            key_hash = self._get_key_hash(cache_key)
            ttl = ttl_seconds or self.ttl_seconds
            
            try:
                # ë°ì´í„° íŒŒì¼ ì €ì¥
                data_file = self._save_data_file(key_hash, data)
                data_size = (self.cache_dir / data_file).stat().st_size
                
                # ë©”íƒ€ë°ì´í„° ì €ì¥
                with sqlite3.connect(self.db_path) as conn:
                    # L1 ìºì‹œ ì €ì¥
                    conn.execute("""
                        INSERT OR REPLACE INTO cache_entries 
                        (key_hash, model, temperature, prompt_hash, query_hash, 
                         created_at, ttl_seconds, hit_count, data_file, data_size)
                        VALUES (?, ?, ?, ?, ?, ?, ?, 0, ?, ?)
                    """, (
                        key_hash,
                        cache_key.model,
                        cache_key.temperature,
                        cache_key.prompt_hash,
                        cache_key.query_hash,
                        datetime.now().isoformat(),
                        ttl,
                        data_file,
                        data_size
                    ))
                    
                    # L2 íŒ¨í„´ ì €ì¥ (ì§ˆë¬¸ì´ ìˆëŠ” ê²½ìš°)
                    if self.enable_l2 and query:
                        self._store_l2_pattern(model, temperature, query, key_hash, conn)
                
                print(f"ğŸ’¾ L1 ìºì‹œ ì €ì¥: {key_hash[:8]}... ({data_size} bytes)")
                return True
                
            except Exception as e:
                print(f"âŒ ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")
                return False
    
    def _store_l2_pattern(self, model: str, temperature: float, query: str, cache_key_hash: str, conn):
        """L2 ì§ˆë¬¸ íŒ¨í„´ ì €ì¥"""
        try:
            normalized_query = self._normalize_query(query)
            keywords = self._get_query_keywords(query)
            keywords_str = ' '.join(keywords)
            
            # íŒ¨í„´ í•´ì‹œ ìƒì„±
            pattern_str = f"{model}:{temperature}:{normalized_query}"
            pattern_hash = hashlib.sha256(pattern_str.encode('utf-8')).hexdigest()[:16]
            
            conn.execute("""
                INSERT OR REPLACE INTO l2_query_patterns 
                (pattern_hash, original_query, normalized_query, query_keywords,
                 model, temperature, cache_key_hash, created_at, similarity_hits)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, 0)
            """, (
                pattern_hash,
                query,
                normalized_query,
                keywords_str,
                model,
                temperature,
                cache_key_hash,
                datetime.now().isoformat()
            ))
            
            print(f"ğŸ”„ L2 íŒ¨í„´ ì €ì¥: {pattern_hash} (í‚¤ì›Œë“œ: {len(keywords)}ê°œ)")
            
        except Exception as e:
            print(f"âš ï¸ L2 íŒ¨í„´ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def _delete_entry(self, key_hash: str, data_file: str):
        """ìºì‹œ ì—”íŠ¸ë¦¬ ì‚­ì œ"""
        try:
            # ë°ì´í„° íŒŒì¼ ì‚­ì œ
            file_path = self.cache_dir / data_file
            if file_path.exists():
                file_path.unlink()
            
            # DB ì—”íŠ¸ë¦¬ ì‚­ì œ
            with sqlite3.connect(self.db_path) as conn:
                conn.execute("DELETE FROM cache_entries WHERE key_hash = ?", (key_hash,))
                
        except Exception as e:
            print(f"âš ï¸ ìºì‹œ ì—”íŠ¸ë¦¬ ì‚­ì œ ì‹¤íŒ¨: {e}")
    
    def cleanup_expired(self) -> int:
        """ë§Œë£Œëœ ìºì‹œ ì—”íŠ¸ë¦¬ ì •ë¦¬"""
        with self._lock:
            deleted_count = 0
            
            try:
                with sqlite3.connect(self.db_path) as conn:
                    cursor = conn.execute("""
                        SELECT key_hash, data_file, created_at, ttl_seconds
                        FROM cache_entries
                    """)
                    
                    expired_entries = []
                    for row in cursor.fetchall():
                        key_hash, data_file, created_at_str, ttl_seconds = row
                        created_at = datetime.fromisoformat(created_at_str)
                        
                        if self._is_expired(created_at, ttl_seconds):
                            expired_entries.append((key_hash, data_file))
                    
                    # ë§Œë£Œëœ ì—”íŠ¸ë¦¬ ì‚­ì œ
                    for key_hash, data_file in expired_entries:
                        self._delete_entry(key_hash, data_file)
                        deleted_count += 1
                
                if deleted_count > 0:
                    print(f"ğŸ§¹ ë§Œë£Œëœ ìºì‹œ ì—”íŠ¸ë¦¬ {deleted_count}ê°œ ì •ë¦¬ ì™„ë£Œ")
                
                return deleted_count
                
            except Exception as e:
                print(f"âŒ ìºì‹œ ì •ë¦¬ ì‹¤íŒ¨: {e}")
                return 0
    
    def get_stats(self) -> Dict[str, Any]:
        """L2 ìºì‹œ í†µê³„ ì¡°íšŒ (ì „ë¬¸ê°€ ì¡°ì–¸ ì§€í‘œ í¬í•¨)"""
        cache_size = 0
        l1_entry_count = 0
        l2_pattern_count = 0
        
        try:
            with sqlite3.connect(self.db_path) as conn:
                # L1 ìºì‹œ í†µê³„
                cursor = conn.execute("SELECT COUNT(*), SUM(data_size) FROM cache_entries")
                row = cursor.fetchone()
                if row:
                    l1_entry_count, total_size = row
                    cache_size = total_size or 0
                
                # L2 íŒ¨í„´ í†µê³„
                cursor = conn.execute("SELECT COUNT(*) FROM l2_query_patterns")
                row = cursor.fetchone()
                if row:
                    l2_pattern_count = row[0]
        except Exception:
            pass
        
        # ì§€ì—°ì‹œê°„ ì ˆì•½ ê³„ì‚° (ì¶”ì •ê°’)
        estimated_latency_saved = self.stats.total_hits * 120  # 120ms per hit
        
        return {
            "total_requests": self.stats.total_requests,
            "l1_hits": self.stats.l1_hits,
            "l2_hits": self.stats.l2_hits,
            "total_hits": self.stats.total_hits,
            "cache_misses": self.stats.cache_misses,
            "hit_rate": self.stats.hit_rate,
            "l2_effectiveness": self.stats.l2_effectiveness,
            "l1_entry_count": l1_entry_count,
            "l2_pattern_count": l2_pattern_count,
            "cache_size_bytes": cache_size,
            "cache_size_mb": cache_size / (1024 * 1024) if cache_size > 0 else 0,
            "estimated_latency_saved_ms": estimated_latency_saved,
            "similarity_threshold": self.similarity_threshold,
            "l2_enabled": self.enable_l2
        }
    
    def clear(self) -> bool:
        """ì „ì²´ ìºì‹œ ì‚­ì œ"""
        with self._lock:
            try:
                # ëª¨ë“  ë°ì´í„° íŒŒì¼ ì‚­ì œ
                for file_path in self.cache_dir.glob("*.pkl"):
                    file_path.unlink()
                
                # DB ì´ˆê¸°í™”
                with sqlite3.connect(self.db_path) as conn:
                    conn.execute("DELETE FROM cache_entries")
                
                # í†µê³„ ë¦¬ì…‹
                self.stats = CacheStats()
                
                print("ğŸ§¹ ì „ì²´ ìºì‹œ ì‚­ì œ ì™„ë£Œ")
                return True
                
            except Exception as e:
                print(f"âŒ ìºì‹œ ì‚­ì œ ì‹¤íŒ¨: {e}")
                return False
    
    def find_similar_queries(self, query: str, model: str, temperature: float, limit: int = 5) -> List[Dict]:
        """ìœ ì‚¬í•œ ì¿¼ë¦¬ ê²€ìƒ‰ (ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜)"""
        query_words = set(query.lower().split())
        similar_entries = []
        
        try:
            with sqlite3.connect(self.db_path) as conn:
                cursor = conn.execute("""
                    SELECT key_hash, query_hash, hit_count, created_at
                    FROM cache_entries 
                    WHERE model = ? AND temperature = ?
                    ORDER BY hit_count DESC
                    LIMIT ?
                """, (model, temperature, limit * 2))  # ë” ë§ì´ ê°€ì ¸ì™€ì„œ í•„í„°ë§
                
                for row in cursor.fetchall():
                    key_hash, query_hash, hit_count, created_at = row
                    
                    # ê°„ë‹¨í•œ ìœ ì‚¬ë„ ê³„ì‚° (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ semantic similarity í•„ìš”)
                    if query_hash:  # query_hashê°€ ìˆëŠ” ê²½ìš°ë§Œ
                        similar_entries.append({
                            "key_hash": key_hash,
                            "query_hash": query_hash,
                            "hit_count": hit_count,
                            "created_at": created_at
                        })
                
                return similar_entries[:limit]
                
        except Exception as e:
            print(f"âš ï¸ ìœ ì‚¬ ì¿¼ë¦¬ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []

class CachedLLMClient:
    """ìºì‹œê°€ ì ìš©ëœ LLM í´ë¼ì´ì–¸íŠ¸ ë˜í¼"""
    
    def __init__(self, llm_client, cache: SemanticCache):
        """ì´ˆê¸°í™”"""
        self.llm_client = llm_client
        self.cache = cache
        print("ğŸ”„ CachedLLMClient ì´ˆê¸°í™” ì™„ë£Œ")
    
    def chat_completion_cached(self, model: str, messages: List[Dict], temperature: float = 0.7, 
                             max_tokens: int = 150, use_cache: bool = True, **kwargs) -> Any:
        """ìºì‹œê°€ ì ìš©ëœ chat completion"""
        
        if not use_cache:
            return self.llm_client.chat.completions.create(
                model=model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
                **kwargs
            )
        
        # í”„ë¡¬í”„íŠ¸ êµ¬ì„± (ìºì‹œ í‚¤ìš©)
        prompt_parts = []
        user_query = ""
        
        for msg in messages:
            if msg["role"] == "system":
                prompt_parts.append(f"SYSTEM: {msg['content']}")
            elif msg["role"] == "user":
                prompt_parts.append(f"USER: {msg['content']}")
                user_query = msg['content']  # ë§ˆì§€ë§‰ user ë©”ì‹œì§€ë¥¼ ì¿¼ë¦¬ë¡œ ì‚¬ìš©
        
        prompt = "\n".join(prompt_parts)
        
        # ìºì‹œì—ì„œ ì¡°íšŒ
        cached_response = self.cache.get(model, temperature, prompt, user_query)
        if cached_response is not None:
            return cached_response
        
        # ìºì‹œ ë¯¸ìŠ¤ - LLM í˜¸ì¶œ
        start_time = time.time()
        response = self.llm_client.chat.completions.create(
            model=model,
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens,
            **kwargs
        )
        call_time = (time.time() - start_time) * 1000
        
        # ì‘ë‹µ ìºì‹œì— ì €ì¥
        self.cache.set(model, temperature, prompt, response, user_query)
        
        print(f"ğŸš€ LLM í˜¸ì¶œ ì™„ë£Œ ({call_time:.1f}ms), ìºì‹œì— ì €ì¥ë¨")
        return response 